{"cells":[{"cell_type":"markdown","source":["# ML4NLP1\n","## Starting Point for Exercise 1, part II\n","\n","This notebook is supposed to serve as a starting point and/or inspiration when starting exercise 1, part II.\n","\n","One of the goals of this exercise is o make you acquainted with **skorch**. You will probably need to consult the [documentation](https://skorch.readthedocs.io/en/stable/)."],"metadata":{"id":"Q-2GcUhgB0S7"}},{"cell_type":"markdown","metadata":{"id":"V920LTuiq40d"},"source":["# Installing skorch and loading libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"utYcb97jq40t","executionInfo":{"status":"ok","timestamp":1697311980987,"user_tz":-120,"elapsed":7460,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"outputs":[],"source":["import subprocess\n","\n","# Installation on Google Colab\n","try:\n","    import google.colab\n","    subprocess.run(['python', '-m', 'pip', 'install', 'skorch'])\n","except ImportError:\n","    pass"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WZ3Y_KHvq40x","executionInfo":{"status":"ok","timestamp":1697311985713,"user_tz":-120,"elapsed":4732,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from skorch import NeuralNetClassifier"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"D9d6X0ZZq40z","executionInfo":{"status":"ok","timestamp":1697311985713,"user_tz":-120,"elapsed":15,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"outputs":[],"source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"H55IvQdyq403","executionInfo":{"status":"ok","timestamp":1697311985713,"user_tz":-120,"elapsed":13,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import csv\n","import re\n","import string\n","from collections import defaultdict"]},{"cell_type":"markdown","metadata":{"id":"dAnY8yaDq400"},"source":["## Training a classifier and making predictions"]},{"cell_type":"code","source":["# download dataset\n","!gdown 1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs # x_train\n","!gdown 1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6 # x_test\n","!gdown 1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl # y_train\n","!gdown 1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X # y_test"],"metadata":{"id":"zWjt9xGoswAC","executionInfo":{"status":"ok","timestamp":1697311995561,"user_tz":-120,"elapsed":9860,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}},"outputId":"52f9a4ae-8d18-4f9e-e1c5-ab679efbd8f4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs\n","To: /content/x_train.txt\n","100% 64.1M/64.1M [00:00<00:00, 245MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6\n","To: /content/x_test.txt\n","100% 65.2M/65.2M [00:00<00:00, 126MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl\n","To: /content/y_train.txt\n","100% 480k/480k [00:00<00:00, 113MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X\n","To: /content/y_test.txt\n","100% 480k/480k [00:00<00:00, 58.8MB/s]\n"]}]},{"cell_type":"code","source":["with open(f'x_train.txt') as f:\n","    x_train = f.read().splitlines()\n","with open(f'y_train.txt') as f:\n","    y_train = f.read().splitlines()\n","with open(f'x_test.txt') as f:\n","    x_test = f.read().splitlines()\n","with open(f'y_test.txt') as f:\n","    y_test = f.read().splitlines()"],"metadata":{"id":"-M6DgXdjtJyH","executionInfo":{"status":"ok","timestamp":1697311998705,"user_tz":-120,"elapsed":3157,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# combine x_train and y_train into one dataframe\n","train_df = pd.DataFrame({'text': x_train, 'label': y_train})\n","\n","#combine x_test and y_test into one dataframe\n","test_df = pd.DataFrame({'text': x_test, 'label': y_test})"],"metadata":{"id":"oRqfDA9FuoX1","executionInfo":{"status":"ok","timestamp":1697311998705,"user_tz":-120,"elapsed":15,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# T: Please use again the train/test data that includes English, German, Dutch, Danish, Swedish and Norwegian, plus 20 additional languages of your choice (the labels can be found in the file labels.csv)\n","# and adjust the train/test split if needed\n","from sklearn.model_selection import train_test_split\n","\n","l = train_df['label'].unique().tolist()\n","l.sort()\n","\n","\n","all_df = pd.concat([train_df, test_df], ignore_index=True)\n","\n","\n","# pre selected langauges\n","languages = ['eng', 'deu', 'nld', 'dan', 'swe', 'nob']\n","\n","# get 20 other langauges randomly\n","num_languages = 20\n","l = [lang for lang in l if lang not in languages]\n","np.random.seed(42)\n","rand_idx = np.random.choice(len(l), num_languages)\n","\n","for i in range(num_languages):\n","  idx = rand_idx[i]\n","  languages.append(l[idx])\n","\n","# now use this list to get corresponding training data\n","new_df = all_df[all_df['label'].isin(languages)]\n","train_x, test_x, train_y, test_y= train_test_split(new_df['text'], new_df['label'], test_size = 0.2, random_state = 42, stratify = new_df['label'])\n","\n"],"metadata":{"id":"r2cICoZ8xNMM","executionInfo":{"status":"ok","timestamp":1697311999113,"user_tz":-120,"elapsed":421,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# T: use your adjusted code to encode the labels here\n","\n","from sklearn.preprocessing import LabelEncoder\n","# le_fitted = LabelEncoder().fit(train_df['label'])\n","# y_train_dev, y_test = le_fitted.fit(train_df['label']), le_fitted.fit(test_df['label'])\n","\n","le_fitted = LabelEncoder().fit(train_y)\n","y_train_dev = le_fitted.transform(train_y)\n","y_test = le_fitted.transform(test_y)"],"metadata":{"id":"PXpIOpjRxzTl","executionInfo":{"status":"ok","timestamp":1697311999113,"user_tz":-120,"elapsed":4,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# T: In the following, you can find a small (almost) working example of a neural network. Unfortunately, again, the cat messed up some of the code. Please fix the code such that it is executable."],"metadata":{"id":"212FI4CFFnrS","executionInfo":{"status":"ok","timestamp":1697311999113,"user_tz":-120,"elapsed":4,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# First, we extract some simple features as input for the neural network\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2), max_features=500,)\n","X = vectorizer.fit_transform(train_x.to_numpy())"],"metadata":{"id":"2-Ls0e0GQgMF","executionInfo":{"status":"ok","timestamp":1697312105066,"user_tz":-120,"elapsed":4911,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["X = X.astype(np.float32)\n","y = y_train_dev.astype(np.int64)"],"metadata":{"id":"9EiRal_1Q0iJ","executionInfo":{"status":"ok","timestamp":1697312006758,"user_tz":-120,"elapsed":17,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMFoiitJq407"},"source":["In the following, we define a vanilla neural network with two hidden layers. The output layer should have as many outputs as there are classes. In addition, it should have a nonlinearity function."]},{"cell_type":"code","source":["class ClassifierModule(nn.Module):\n","    def __init__(\n","            self,\n","            num_units=200,\n","            nonlin=F.relu,\n","    ):\n","        super(ClassifierModule, self).__init__()\n","        self.num_units = num_units\n","        self.nonlin = nonlin\n","\n","        self.dense0 = nn.Linear(500, num_units)\n","        self.nonlin = nonlin\n","        self.dense1 = nn.Linear(num_units, 250)\n","        self.dense2 = nn.Linear(250, 100)\n","        self.output = nn.Linear(100, 26)\n","\n","\n","    def forward(self, X, **kwargs):\n","      X = self.nonlin(self.dense0(X))\n","\n","      X = self.nonlin(self.dense1(X))\n","      X = self.nonlin(self.dense2(X))\n","      X = self.output(X)\n","      return X.squeeze(dim=1)"],"metadata":{"id":"7Q5EDIGQUUBy","executionInfo":{"status":"ok","timestamp":1697312006758,"user_tz":-120,"elapsed":16,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"yh4xz_3jelIo"}},{"cell_type":"code","source":["net = NeuralNetClassifier(\n","    ClassifierModule,\n","    max_epochs=20,\n","    criterion=nn.CrossEntropyLoss(),\n","    lr=0.1,\n","    device='cuda',  # comment this to train with CPU\n",")"],"metadata":{"id":"wKnJECeQGpyI","executionInfo":{"status":"ok","timestamp":1697312006758,"user_tz":-120,"elapsed":16,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["net.fit(X, y)"],"metadata":{"id":"QcNOd9yBSxys","executionInfo":{"status":"ok","timestamp":1697312050743,"user_tz":-120,"elapsed":44001,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}},"outputId":"12193f95-94d6-42e7-c5e7-a7613e265e76","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m1.4489\u001b[0m       \u001b[32m0.9357\u001b[0m        \u001b[35m0.3554\u001b[0m  2.4107\n","      2        \u001b[36m0.2920\u001b[0m       \u001b[32m0.9575\u001b[0m        \u001b[35m0.2032\u001b[0m  2.0235\n","      3        \u001b[36m0.1869\u001b[0m       \u001b[32m0.9640\u001b[0m        \u001b[35m0.1698\u001b[0m  1.9718\n","      4        \u001b[36m0.1394\u001b[0m       \u001b[32m0.9670\u001b[0m        \u001b[35m0.1515\u001b[0m  1.9670\n","      5        \u001b[36m0.1181\u001b[0m       \u001b[32m0.9692\u001b[0m        \u001b[35m0.1436\u001b[0m  2.7986\n","      6        \u001b[36m0.0960\u001b[0m       \u001b[32m0.9710\u001b[0m        \u001b[35m0.1377\u001b[0m  1.9043\n","      7        \u001b[36m0.0846\u001b[0m       \u001b[32m0.9718\u001b[0m        \u001b[35m0.1340\u001b[0m  1.9962\n","      8        \u001b[36m0.0761\u001b[0m       \u001b[32m0.9720\u001b[0m        \u001b[35m0.1311\u001b[0m  1.9780\n","      9        \u001b[36m0.0688\u001b[0m       0.9720        \u001b[35m0.1299\u001b[0m  1.9624\n","     10        \u001b[36m0.0626\u001b[0m       0.9720        \u001b[35m0.1295\u001b[0m  1.9654\n","     11        \u001b[36m0.0580\u001b[0m       \u001b[32m0.9725\u001b[0m        0.1304  2.8757\n","     12        \u001b[36m0.0514\u001b[0m       0.9720        0.1313  1.9812\n","     13        \u001b[36m0.0492\u001b[0m       0.9715        0.1336  1.9620\n","     14        \u001b[36m0.0442\u001b[0m       0.9712        0.1353  2.0119\n","     15        \u001b[36m0.0389\u001b[0m       0.9722        0.1376  1.9500\n","     16        \u001b[36m0.0354\u001b[0m       0.9725        0.1400  2.0844\n","     17        \u001b[36m0.0322\u001b[0m       0.9718        0.1422  2.7625\n","     18        \u001b[36m0.0276\u001b[0m       0.9725        0.1453  2.0012\n","     19        \u001b[36m0.0256\u001b[0m       0.9722        0.1472  1.9653\n","     20        \u001b[36m0.0217\u001b[0m       \u001b[32m0.9728\u001b[0m        0.1497  2.0158\n"]},{"output_type":"execute_result","data":{"text/plain":["<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n","  module_=ClassifierModule(\n","    (dense0): Linear(in_features=500, out_features=200, bias=True)\n","    (dense1): Linear(in_features=200, out_features=250, bias=True)\n","    (dense2): Linear(in_features=250, out_features=100, bias=True)\n","    (output): Linear(in_features=100, out_features=26, bias=True)\n","  ),\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["X = vectorizer.transform(test_x.to_numpy())\n","X = X.astype(np.float32)\n","y = y_test.astype(np.int64)\n","y_test\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzntO6YUaoh_","executionInfo":{"status":"ok","timestamp":1697312105860,"user_tz":-120,"elapsed":798,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}},"outputId":"fdcac79c-ed65-442c-bee2-815194dde898"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([16, 16, 24, ..., 17, 11,  3])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["y_pred = net.predict(X)"],"metadata":{"id":"TEi_kUJsrup6","executionInfo":{"status":"ok","timestamp":1697312143143,"user_tz":-120,"elapsed":1133,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["accuraccy = y_pred == y_test"],"metadata":{"id":"GyNTgM8wr8xa","executionInfo":{"status":"ok","timestamp":1697312155295,"user_tz":-120,"elapsed":3,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["accuraccy.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kn-fnZwKsCOB","executionInfo":{"status":"ok","timestamp":1697312166019,"user_tz":-120,"elapsed":5,"user":{"displayName":"Muhummad Hamza","userId":"08136739869967620158"}},"outputId":"dd17e67c-5102-4119-f5e1-25ac3fce693a"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.973"]},"metadata":{},"execution_count":24}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"bd97b8bffa4d3737e84826bc3d37be3046061822757ce35137ab82ad4c5a2016"}},"colab":{"provenance":[{"file_id":"1wDtacsvGOiJd2SRjqS763isiz2LVziFP","timestamp":1696939814229},{"file_id":"1vwyB2s07x98ecayW6bfAx828qZ67Bgfp","timestamp":1696144289653},{"file_id":"https://github.com/skorch-dev/skorch/blob/master/notebooks/Basic_Usage.ipynb","timestamp":1695936748288}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}