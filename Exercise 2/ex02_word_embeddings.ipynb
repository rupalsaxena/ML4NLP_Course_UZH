{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wvrt-REqt5ez"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMv2k-eFt5e5"
   },
   "source": [
    "Source: [https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)\n",
    "\n",
    "# Word Embeddings: Encoding Lexical Semantics\n",
    "\n",
    "Word embeddings are dense vectors of real numbers, one per word in your\n",
    "vocabulary. In NLP, it is almost always the case that your features are\n",
    "words! But how should you represent a word in a computer? You could\n",
    "store its ascii character representation, but that only tells you what\n",
    "the word *is*, it doesn't say much about what it *means* (you might be\n",
    "able to derive its part of speech from its affixes, or properties from\n",
    "its capitalization, but not much). Even more, in what sense could you\n",
    "combine these representations? We often want dense outputs from our\n",
    "neural networks, where the inputs are $|V|$ dimensional, where\n",
    "$V$ is our vocabulary, but often the outputs are only a few\n",
    "dimensional (if we are only predicting a handful of labels, for\n",
    "instance). How do we get from a massive dimensional space to a smaller\n",
    "dimensional space?\n",
    "\n",
    "How about instead of ascii representations, we use a one-hot encoding?\n",
    "That is, we represent the word $w$ by\n",
    "\n",
    "\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n",
    "\n",
    "where the 1 is in a location unique to $w$. Any other word will\n",
    "have a 1 in some other location, and a 0 everywhere else.\n",
    "\n",
    "There is an enormous drawback to this representation, besides just how\n",
    "huge it is. It basically treats all words as independent entities with\n",
    "no relation to each other. What we really want is some notion of\n",
    "*similarity* between words. Why? Let's see an example.\n",
    "\n",
    "Suppose we are building a language model. Suppose we have seen the\n",
    "sentences\n",
    "\n",
    "* The mathematician ran to the store.\n",
    "* The physicist ran to the store.\n",
    "* The mathematician solved the open problem.\n",
    "\n",
    "in our training data. Now suppose we get a new sentence never before\n",
    "seen in our training data:\n",
    "\n",
    "* The physicist solved the open problem.\n",
    "\n",
    "Our language model might do OK on this sentence, but wouldn't it be much\n",
    "better if we could use the following two facts:\n",
    "\n",
    "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n",
    "  have a semantic relation.\n",
    "* We have seen mathematician in the same role  in this new unseen sentence\n",
    "  as we are now seeing physicist.\n",
    "\n",
    "and then infer that physicist is actually a good fit in the new unseen\n",
    "sentence? This is what we mean by a notion of similarity: we mean\n",
    "*semantic similarity*, not simply having similar orthographic\n",
    "representations. It is a technique to combat the sparsity of linguistic\n",
    "data, by connecting the dots between what we have seen and what we\n",
    "haven't. This example of course relies on a fundamental linguistic\n",
    "assumption: that words appearing in similar contexts are related to each\n",
    "other semantically. This is called the `distributional\n",
    "hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
    "\n",
    "\n",
    "# Getting Dense Word Embeddings\n",
    "\n",
    "How can we solve this problem? That is, how could we actually encode\n",
    "semantic similarity in words? Maybe we think up some semantic\n",
    "attributes. For example, we see that both mathematicians and physicists\n",
    "can run, so maybe we give these words a high score for the \"is able to\n",
    "run\" semantic attribute. Think of some other attributes, and imagine\n",
    "what you might score some common words on those attributes.\n",
    "\n",
    "If each attribute is a dimension, then we might give each word a vector,\n",
    "like this:\n",
    "\n",
    "\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
    "   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
    "\n",
    "\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
    "   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
    "\n",
    "Then we can get a measure of similarity between these words by doing:\n",
    "\n",
    "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n",
    "\n",
    "Although it is more common to normalize by the lengths:\n",
    "\n",
    "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
    "   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n",
    "\n",
    "Where $\\phi$ is the angle between the two vectors. That way,\n",
    "extremely similar words (words whose embeddings point in the same\n",
    "direction) will have similarity 1. Extremely dissimilar words should\n",
    "have similarity -1.\n",
    "\n",
    "\n",
    "You can think of the sparse one-hot vectors from the beginning of this\n",
    "section as a special case of these new vectors we have defined, where\n",
    "each word basically has similarity 0, and we gave each word some unique\n",
    "semantic attribute. These new vectors are *dense*, which is to say their\n",
    "entries are (typically) non-zero.\n",
    "\n",
    "But these new vectors are a big pain: you could think of thousands of\n",
    "different semantic attributes that might be relevant to determining\n",
    "similarity, and how on earth would you set the values of the different\n",
    "attributes? Central to the idea of deep learning is that the neural\n",
    "network learns representations of the features, rather than requiring\n",
    "the programmer to design them herself. So why not just let the word\n",
    "embeddings be parameters in our model, and then be updated during\n",
    "training? This is exactly what we will do. We will have some *latent\n",
    "semantic attributes* that the network can, in principle, learn. Note\n",
    "that the word embeddings will probably not be interpretable. That is,\n",
    "although with our hand-crafted vectors above we can see that\n",
    "mathematicians and physicists are similar in that they both like coffee,\n",
    "if we allow a neural network to learn the embeddings and see that both\n",
    "mathematicians and physicists have a large value in the second\n",
    "dimension, it is not clear what that means. They are similar in some\n",
    "latent semantic dimension, but this probably has no interpretation to\n",
    "us.\n",
    "\n",
    "\n",
    "In summary, **word embeddings are a representation of the *semantics* of\n",
    "a word, efficiently encoding semantic information that might be relevant\n",
    "to the task at hand**. You can embed other things too: part of speech\n",
    "tags, parse trees, anything! The idea of feature embeddings is central\n",
    "to the field.\n",
    "\n",
    "\n",
    "# Word Embeddings in Pytorch\n",
    "\n",
    "Before we get to a worked example and an exercise, a few quick notes\n",
    "about how to use embeddings in Pytorch and in deep learning programming\n",
    "in general. Similar to how we defined a unique index for each word when\n",
    "making one-hot vectors, we also need to define an index for each word\n",
    "when using embeddings. These will be keys into a lookup table. That is,\n",
    "embeddings are stored as a $|V| \\times D$ matrix, where $D$\n",
    "is the dimensionality of the embeddings, such that the word assigned\n",
    "index $i$ has its embedding stored in the $i$'th row of the\n",
    "matrix. In all of my code, the mapping from words to indices is a\n",
    "dictionary named word\\_to\\_ix.\n",
    "\n",
    "The module that allows you to use embeddings is torch.nn.Embedding,\n",
    "which takes two arguments: the vocabulary size, and the dimensionality\n",
    "of the embeddings.\n",
    "\n",
    "To index into this table, you must use torch.LongTensor (since the\n",
    "indices are integers, not floats).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELBc1ntbt5e8",
    "outputId": "4739f1c9-133e-4f22-8fd6-3f4a6948e3d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f74527d8190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcvzqik4t5e9",
    "outputId": "07c31377-3902-4946-880e-195933c8fffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
       "         [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
       "\n",
       "        [[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631],\n",
       "         [ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "print(lookup_tensor)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)\n",
    "\n",
    "embeds(torch.tensor([[0,1], [1,0]], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "es54fI4yt5e-"
   },
   "source": [
    "# An Example: N-Gram Language Modeling\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words\n",
    "$w$, we want to compute\n",
    "\n",
    "\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n",
    "\n",
    "Where $w_i$ is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training\n",
    "examples and update the parameters with backpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKEd73J5t5e_",
    "outputId": "7f21027e-a9ae-46c0-ea93-fcf1f3a93c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "[520.363648891449, 517.6042952537537, 514.8673779964447, 512.1517074108124, 509.45520973205566, 506.77791261672974, 504.11913299560547, 501.4802691936493, 498.85717129707336, 496.24901127815247]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO-_4P3mt5e_"
   },
   "source": [
    "# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
    "\n",
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
    "learning. It is a model that tries to predict words given the context of\n",
    "a few words before and a few words after the target word. This is\n",
    "distinct from language modeling, since CBOW is not sequential and does\n",
    "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
    "word embeddings, and these embeddings are used to initialize the\n",
    "embeddings of some more complicated model. Usually, this is referred to\n",
    "as *pretraining embeddings*. It almost always helps performance a couple\n",
    "of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an\n",
    "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
    "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
    "collectively as $C$, CBOW tries to minimize\n",
    "\n",
    "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
    "\n",
    "where $q_w$ is the embedding for word $w$.\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some\n",
    "tips:\n",
    "\n",
    "* Think about which parameters you need to define.\n",
    "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
    "  reshape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMITozAB3aye"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXbt-utMt5fA",
    "outputId": "ea1a3c7d-9945-4a86-bdcb-15aeebf85b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "def generate_batches(data, batch_size=8):\n",
    "    X = [data[i][0] for i in range(len(data))]\n",
    "    y = [data[i][1] for i in range(len(data))]\n",
    "    num_batches = len(data)//batch_size\n",
    "    max_idx = batch_size * num_batches\n",
    "    X = X[:max_idx]\n",
    "    y = y[:max_idx]\n",
    "    X_batched = []\n",
    "    y_batched = []\n",
    "    for i in range(num_batches):\n",
    "      X_batched.append(X[i*batch_size:(i+1)*batch_size])\n",
    "      y_batched.append(y[i*batch_size:(i+1)*batch_size])\n",
    "    return X_batched, y_batched\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n",
    "      super(CBOW, self).__init__()\n",
    "      # Embedding layer - Lookup table\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "      # Layer 1 - Since we'll be summing up the context vectors, the input to this layer will be embedding_dim. Output is a 256 dim. vector\n",
    "      self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "      # Adding non-linearity through ReLU\n",
    "      self.relu = nn.ReLU()\n",
    "      # Final layer to get to vocab size dim\n",
    "      self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "      # Log softmax to get probabilities\n",
    "      self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      # inputs are context vectors. Get embeddings for them\n",
    "      x = self.embeddings(inputs)\n",
    "      # sum all context vectors\n",
    "      x = torch.sum(x,axis=1).view(inputs.shape[0],-1)\n",
    "      #x = sum(x).view(1,-1)\n",
    "      # Add first layer\n",
    "      x = self.linear1(x)\n",
    "      # Add relu\n",
    "      x = self.relu(x)\n",
    "      # Add final layer\n",
    "      x = self.linear2(x)\n",
    "      # Get log softmax\n",
    "      x = self.log_softmax(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Function to get context vectors with batched data\n",
    "def make_context_vector2(context_list, word_to_ix):\n",
    "    idxs = []\n",
    "    for context in context_list:\n",
    "      idxs.append([word_to_ix[w] for w in context])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Function to get context vectors with batched data\n",
    "def make_labels_idx(labels, word_to_ix):\n",
    "    idxs = []\n",
    "    for label in labels:\n",
    "      idxs.append(word_to_ix[label])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, vocab, device, word_to_ix = word_to_ix, NUM_EPOCHS=15):\n",
    "  losses = []\n",
    "  loss_function = nn.NLLLoss()\n",
    "  model = model.to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "  X_batched, y_batched = generate_batches(data, batch_size=8)\n",
    "\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "      total_loss = 0\n",
    "      for context, target in tqdm(zip(X_batched, y_batched), total=len(X_batched)):\n",
    "\n",
    "          # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "          # into integer indices and wrap them in tensors)\n",
    "          context_idxs = make_context_vector2(context, word_to_ix).to(device)\n",
    "          target_idxs = make_labels_idx(target, word_to_ix).to(device)\n",
    "\n",
    "          # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "          # new instance, you need to zero out the gradients from the old\n",
    "          # instance\n",
    "          model.zero_grad()\n",
    "\n",
    "          # Step 3. Run the forward pass, getting log probabilities over next\n",
    "          # words\n",
    "          log_probs = model(context_idxs)\n",
    "\n",
    "          # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "          # word wrapped in a tensor)\n",
    "          loss = loss_function(log_probs, target_idxs)\n",
    "\n",
    "          # Step 5. Do the backward pass and update the gradient\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "          total_loss += loss.item()\n",
    "\n",
    "      print(\"Loss Epoch {ep} = {ls}\".format(ep = epoch, ls = total_loss))\n",
    "      losses.append(total_loss)\n",
    "  #print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOeuGoZI8bay",
    "outputId": "01f2cca7-0a6a-4045-a2a2-3674edadf6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 0 = 28.088960647583008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 688.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 1 = 27.926408529281616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 627.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 2 = 27.76490068435669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 693.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 3 = 27.604418992996216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 653.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 4 = 27.44498109817505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 673.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 5 = 27.286572217941284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 716.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 6 = 27.12918996810913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 647.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 7 = 26.972814083099365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 675.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 8 = 26.81743288040161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 667.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 9 = 26.66304850578308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 643.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 10 = 26.509656190872192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 677.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 11 = 26.35727095603943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 503.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 12 = 26.20590591430664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 543.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 13 = 26.055581092834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 625.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 14 = 25.90627360343933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 508.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 15 = 25.757943391799927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 614.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 16 = 25.610628366470337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 508.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 17 = 25.46427321434021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 368.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 18 = 25.318897485733032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 447.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 19 = 25.174449682235718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 704.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 20 = 25.030962705612183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 667.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 21 = 24.888399600982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 646.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 22 = 24.746774673461914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 653.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 23 = 24.606098175048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 673.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 24 = 24.466334342956543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 678.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 25 = 24.327523231506348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 657.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 26 = 24.189621925354004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 656.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 27 = 24.05262804031372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 496.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 28 = 23.916526079177856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 486.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 29 = 23.781309366226196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 500.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 30 = 23.646979093551636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 504.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 31 = 23.513529539108276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 425.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 32 = 23.380939722061157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 456.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 33 = 23.249220371246338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 641.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 34 = 23.118361949920654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 565.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 35 = 22.988409757614136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 667.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 36 = 22.859317541122437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 659.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 37 = 22.731013536453247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 604.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 38 = 22.603514432907104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 557.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 39 = 22.47684383392334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 571.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 40 = 22.35092568397522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 475.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 41 = 22.225877285003662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 441.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 42 = 22.10160994529724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 430.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 43 = 21.978124141693115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 484.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 44 = 21.855432748794556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 468.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 45 = 21.73341178894043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 480.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 46 = 21.612124919891357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 447.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 47 = 21.491501092910767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 388.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 48 = 21.371620893478394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 451.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 49 = 21.25249695777893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = CBOW(vocab_size = len(vocab), embedding_dim=50)\n",
    "\n",
    "# Device for GPU Training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"Training on GPU\")\n",
    "   device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Train\n",
    "train(model, data, vocab, device, NUM_EPOCHS=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgS76yPycuEc"
   },
   "source": [
    "## Part 1 - Training CBOW for Trip Advisor and Sci-Fi Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8bjktikeGFZ",
    "outputId": "407d302a-4969-4fe5-bc51-be359bb97d5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGPi0WeQc1nQ",
    "outputId": "808dc7ed-1c8d-4c66-de43-a3c4fe64a482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "                                              Review  Rating\n",
      "0  fantastic service large hotel caters business ...       5\n",
      "1  great hotel modern hotel good location, locate...       4\n",
      "2  3 star plus glasgowjust got 30th november 4 da...       4\n",
      "3  nice stayed hotel nov 19-23. great little bout...       4\n",
      "4  great place wonderful hotel ideally located me...       5\n",
      "Scifi Text\n",
      " A chat with the editor  i #  science fiction magazine called IF. The title was selected after much thought because of its brevity and on the theory it is indicative of the field and will be easy to remember. The tentative title that just morning and couldn't remember it until we'd had a cup of coffee, it was summarily discarded. A great deal of thought and effort lias gone into the formation of this magazine. We have had the aid of several very talented and generous people, for which we are most grateful. Much is due them for their warmhearted assistance. And now that the bulk of the formative work is done, we will try to maintain IF as one of the finest books on the market.  t a great public demand for our magazine. In short, why will you buy IF? We cannot, in honesty, say we will publish at all times the best science fiction in the field. That would not be true. But we will have access to the best stories, and we will get our fair share of works from the best writers. We definitely \n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "\n",
    "\"\"\"\n",
    "trip_advisor_url = https://drive.google.com/file/d/1foE1JuZJeu5E_4qVge9kExzhvF32teuF/view\n",
    "scifi_url = https://drive.google.com/file/d/13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75/view\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change the paths accordingly\n",
    "# trip_advisor_path = '/content/drive/MyDrive/mlnlp1/exercise-2/data/tripadvisor_hotel_reviews_reduced.csv'\n",
    "# scifi_path = '/content/drive/MyDrive/mlnlp1/exercise-2/data/scifi_reduced.txt'\n",
    "trip_advisor_path = 'tripadvisor_hotel_reviews_reduced.csv'\n",
    "scifi_path = 'scifi_reduced.txt'\n",
    "\n",
    "# DF trip advisor\n",
    "df_trip = pd.read_csv(trip_advisor_path)\n",
    "print(df_trip.head())\n",
    "\n",
    "# Scifi text\n",
    "text_file = open(scifi_path, \"r\")\n",
    "scifi_text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "print(\"Scifi Text\")\n",
    "print(scifi_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jJbGvDVeLcJ"
   },
   "outputs": [],
   "source": [
    "# Function to pre-process\n",
    "# 1. Lowercase text\n",
    "# 2. Tokenize based on sentences - split on \".\" - we'll get a list of sentences\n",
    "# 3. For each sentence from 2, tokenize based on punctuations - nltk wordpunct_tokenize - we'll get a list of list of words in a sentence\n",
    "# 4. Remove punctuations\n",
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text_sent_token = text.split(\".\")\n",
    "  text_punct_token = [ wordpunct_tokenize(sent) for sent in text_sent_token]\n",
    "  text_punct_token_cleaned = []\n",
    "  # Remove punctuations from tokenized list of lists.\n",
    "  for txt_list in text_punct_token:\n",
    "    clean_txt = []\n",
    "    for txt in txt_list:\n",
    "      if txt not in string.punctuation:\n",
    "        clean_txt.append(txt)\n",
    "    # Take only texts having more than 1 element\n",
    "    if len(clean_txt)>1:\n",
    "       text_punct_token_cleaned.append(clean_txt)\n",
    "  return text_punct_token_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzAkt6mzsgvG"
   },
   "outputs": [],
   "source": [
    "# Function to get vocab\n",
    "# Input will be a list of lists\n",
    "# Output - vocab set\n",
    "def get_vocab(raw_llist):\n",
    "  vocab = set()\n",
    "  for lst in raw_llist:\n",
    "    for el in lst:\n",
    "      vocab.add(el)\n",
    "  return vocab\n",
    "\n",
    "# Function to get word-to-ix dictionary\n",
    "def get_word2ix(vocab, save_loc):\n",
    "  word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "  with open(save_loc, 'wb') as handle:\n",
    "    pickle.dump(word_to_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc_2-YDNvkVa"
   },
   "outputs": [],
   "source": [
    "# Function to generate tuples of context-target\n",
    "# Input = list of lists\n",
    "# Output = list of tuples (list of context_words, target)\n",
    "def get_data(raw_llist, context_window=5):\n",
    "  data = []\n",
    "  for raw_list in raw_llist:\n",
    "    for i in range(context_window, len(raw_list) - context_window):\n",
    "      context = []\n",
    "      if i-context_window>=0 and i+context_window<len(raw_list):\n",
    "        for j in range(i-context_window, i+context_window+1):\n",
    "          if j!=i:\n",
    "            context.append(raw_list[j])\n",
    "        target = raw_list[i]\n",
    "      data.append((context, target))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R86H7ZU2nAn"
   },
   "outputs": [],
   "source": [
    "# Copy - Paste from previous code (for simplicity)\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n",
    "      super(CBOW, self).__init__()\n",
    "      # Embedding layer - Lookup table\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "      # Layer 1 - Since we'll be summing up the context vectors,\n",
    "      # the input to this layer will be embedding_dim. Output is a 256 dim. vector\n",
    "      self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "      # Adding non-linearity through ReLU\n",
    "      self.relu = nn.ReLU()\n",
    "      # Final layer to get to vocab size dim\n",
    "      self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "      # Log softmax to get probabilities\n",
    "      self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      # inputs are context vectors. Get embeddings for them\n",
    "      x = self.embeddings(inputs)\n",
    "      # sum all context vectors\n",
    "      x = torch.sum(x,axis=1).view(inputs.shape[0],-1)\n",
    "      #x = sum(x).view(1,-1)\n",
    "      # Add first layer\n",
    "      x = self.linear1(x)\n",
    "      # Add relu\n",
    "      x = self.relu(x)\n",
    "      # Add final layer\n",
    "      x = self.linear2(x)\n",
    "      # Get log softmax\n",
    "      x = self.log_softmax(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Function to get context vectors with batched data\n",
    "def make_context_vector2(context_list, word_to_ix):\n",
    "    idxs = []\n",
    "    for context in context_list:\n",
    "      idxs.append([word_to_ix[w] for w in context])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Function to get context vectors with batched data\n",
    "def make_labels_idx(labels, word_to_ix):\n",
    "    idxs = []\n",
    "    for label in labels:\n",
    "      idxs.append(word_to_ix[label])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Function to train the model - minor changes in arguments\n",
    "def train(model, data, vocab, device, word_to_ix, NUM_EPOCHS=15):\n",
    "  losses = []\n",
    "  loss_function = nn.NLLLoss()\n",
    "  model = model.to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "  X_batched, y_batched = generate_batches(data, batch_size=16)\n",
    "\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "      total_loss = 0\n",
    "      for context, target in tqdm(zip(X_batched, y_batched), total=len(X_batched)):\n",
    "\n",
    "          # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "          # into integer indices and wrap them in tensors)\n",
    "          context_idxs = make_context_vector2(context, word_to_ix).to(device)\n",
    "          target_idxs = make_labels_idx(target, word_to_ix).to(device)\n",
    "\n",
    "          # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "          # new instance, you need to zero out the gradients from the old\n",
    "          # instance\n",
    "          model.zero_grad()\n",
    "\n",
    "          # Step 3. Run the forward pass, getting log probabilities over next\n",
    "          # words\n",
    "          log_probs = model(context_idxs)\n",
    "\n",
    "          # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "          # word wrapped in a tensor)\n",
    "          loss = loss_function(log_probs, target_idxs)\n",
    "\n",
    "          # Step 5. Do the backward pass and update the gradient\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "          total_loss += loss.item()\n",
    "\n",
    "      print(\"Loss Epoch {ep} = {ls}\".format(ep = epoch, ls = total_loss))\n",
    "      losses.append(total_loss)\n",
    "  print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2Pg-6sZ3Vcs"
   },
   "source": [
    "### Trip-Advisor Training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqaMg2q03b4F"
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "trip_pp = df_trip[\"Review\"].apply(lambda x: preprocess(x))\n",
    "\n",
    "# Convert to list of lists\n",
    "trip_pp_llist = []\n",
    "trip_pp_rows = trip_pp.tolist()\n",
    "for trip_pp_row in trip_pp_rows:\n",
    "  for trip_pp_row_list in trip_pp_row:\n",
    "    trip_pp_llist.append(trip_pp_row_list)\n",
    "\n",
    "# Vocab\n",
    "trip_vocab = get_vocab(trip_pp_llist)\n",
    "\n",
    "# Word2Ix\n",
    "trip_word2ix = get_word2ix(trip_vocab, '/content/drive/MyDrive/mlnlp1/exercise-2/trip_word2ix.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffrz0Bkg63P3"
   },
   "source": [
    "#### Context-window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjO5SbVX3cqG",
    "outputId": "2e750a0c-29a8-48a7-e17d-0cd5c7221561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  973973\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 2\n",
    "trip_data = get_data(trip_pp_llist, CONTEXT_WINDOW)\n",
    "\n",
    "# Check data has correct shape\n",
    "for data_sample in trip_data:\n",
    "  assert(len(data_sample[0])==2*CONTEXT_WINDOW)\n",
    "\n",
    "print(\"Number of samples: \", len(trip_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKJAur8v7bIP",
    "outputId": "2f92abb0-0f51-48a4-ed59-683f5123ac5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:07<00:00, 478.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 0 = 537733.371711731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:01<00:00, 502.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 1 = 483126.46018075943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:05<00:00, 484.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 2 = 468764.9251241684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:05<00:00, 484.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 3 = 461329.4310104847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 507.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 4 = 456403.26427435875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 508.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 5 = 452727.00867414474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 509.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 6 = 449790.8506155014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 509.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 7 = 447334.53298544884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 509.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 8 = 445208.526365757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:00<00:00, 507.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 9 = 443322.48322701454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 509.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 10 = 441617.5093655586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:00<00:00, 506.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 11 = 440053.86068558693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:00<00:00, 503.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 12 = 438603.6401154995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [02:00<00:00, 506.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 13 = 437245.2623603344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60873/60873 [01:59<00:00, 508.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 14 = 435962.96954774857\n",
      "[537733.371711731, 483126.46018075943, 468764.9251241684, 461329.4310104847, 456403.26427435875, 452727.00867414474, 449790.8506155014, 447334.53298544884, 445208.526365757, 443322.48322701454, 441617.5093655586, 440053.86068558693, 438603.6401154995, 437245.2623603344, 435962.96954774857]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "trip_model_window2 = CBOW(vocab_size = len(trip_vocab), embedding_dim=50)\n",
    "\n",
    "# Device for GPU Training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"Training on GPU\")\n",
    "   device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Train\n",
    "train(model = trip_model_window2, data=trip_data, vocab=trip_vocab, device=device, word_to_ix=trip_word2ix, NUM_EPOCHS=15)\n",
    "torch.save(trip_model_window2, '/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5IgvauEaABe"
   },
   "outputs": [],
   "source": [
    "trip_model_window2_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGmX-nkEaw3_"
   },
   "source": [
    "#### Context-window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyvs1NDTa2IA",
    "outputId": "b33d5616-62d9-497a-b778-482b158eafc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  816667\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 5\n",
    "trip_data_window5 = get_data(trip_pp_llist, CONTEXT_WINDOW)\n",
    "\n",
    "# Check data has correct shape\n",
    "for data_sample in trip_data_window5:\n",
    "  assert(len(data_sample[0])==2*CONTEXT_WINDOW)\n",
    "\n",
    "print(\"Number of samples: \", len(trip_data_window5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WViT1Za0bMlo",
    "outputId": "13344207-ff17-469c-8102-09a434d95484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 504.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 0 = 447444.58891153336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:43<00:00, 494.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 1 = 408827.3837146759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 500.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 2 = 399027.7963979244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 501.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 3 = 393605.24425554276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 502.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 4 = 389891.3892633915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 503.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 5 = 387067.2838578224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:42<00:00, 500.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 6 = 384777.1399919987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 502.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 7 = 382838.22569322586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 500.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 8 = 381145.7957429886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:42<00:00, 499.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 9 = 379634.40681552887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:41<00:00, 502.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 10 = 378260.5902414322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:42<00:00, 497.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 11 = 376993.6868059635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:40<00:00, 505.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 12 = 375812.3440673351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:42<00:00, 498.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 13 = 374700.0394477844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51041/51041 [01:40<00:00, 506.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 14 = 373644.72666954994\n",
      "[447444.58891153336, 408827.3837146759, 399027.7963979244, 393605.24425554276, 389891.3892633915, 387067.2838578224, 384777.1399919987, 382838.22569322586, 381145.7957429886, 379634.40681552887, 378260.5902414322, 376993.6868059635, 375812.3440673351, 374700.0394477844, 373644.72666954994]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "trip_model_window5 = CBOW(vocab_size = len(trip_vocab), embedding_dim=50)\n",
    "\n",
    "# Device for GPU Training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"Training on GPU\")\n",
    "   device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Train\n",
    "train(model = trip_model_window5, data=trip_data_window5, vocab=trip_vocab, device=device, word_to_ix=trip_word2ix,\n",
    "      NUM_EPOCHS=15)\n",
    "# Save model\n",
    "torch.save(trip_model_window5, '/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window5.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1heu7kBcKkx"
   },
   "outputs": [],
   "source": [
    "trip_model_window5_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see different overall losses over the vocabulary when using window size 2 and 5, the predictions made by the model are context sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0882KJz3dSy"
   },
   "source": [
    "### Scifi Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuCj6aB-gGej"
   },
   "source": [
    "#### Context-window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8G7Dphbyrz7",
    "outputId": "1e444041-e64a-4a79-fce3-377cf69a0273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  5749466\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 2\n",
    "\n",
    "scifi_preprocessed = preprocess(scifi_text)\n",
    "scifi_vocab = get_vocab(scifi_preprocessed)\n",
    "scifi_word2ix = get_word2ix(scifi_vocab, '/content/drive/MyDrive/mlnlp1/exercise-2/scifi_word2ix.pickle')\n",
    "scifi_data = get_data(scifi_preprocessed, CONTEXT_WINDOW)\n",
    "print(\"Number of samples: \", len(scifi_data))\n",
    "\n",
    "# Check data has correct shape\n",
    "for data_sample in scifi_data:\n",
    "  assert(len(data_sample[0])==2*CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODLa7kPL08Tz",
    "outputId": "bc22439c-db0e-4aa9-c179-28bf3baf5677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359341/359341 [28:21<00:00, 211.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 0 = 2635441.4196851254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359341/359341 [28:20<00:00, 211.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 1 = 2420369.2964789867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359341/359341 [28:21<00:00, 211.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 2 = 2369389.719114661\n",
      "[2635441.4196851254, 2420369.2964789867, 2369389.719114661]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "scifi_model_window2 = CBOW(vocab_size = len(scifi_vocab), embedding_dim=50)\n",
    "\n",
    "# Device for GPU Training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"Training on GPU\")\n",
    "   device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Train\n",
    "train(model = scifi_model_window2, data=scifi_data, vocab=scifi_vocab, device=device, word_to_ix=scifi_word2ix, NUM_EPOCHS=3)\n",
    "torch.save(scifi_model_window2, '/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j85oDStBXFBp"
   },
   "outputs": [],
   "source": [
    "scifi_model_window2_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_QSH60HgLXu"
   },
   "source": [
    "#### Context-window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mflmVt9hgKSs",
    "outputId": "3c4067da-a331-416c-cea0-37d6e5f56d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  3264696\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_WINDOW = 5\n",
    "scifi_data = get_data(scifi_preprocessed, CONTEXT_WINDOW)\n",
    "print(\"Number of samples: \", len(scifi_data))\n",
    "\n",
    "# Check data has correct shape\n",
    "for data_sample in scifi_data:\n",
    "  assert(len(data_sample[0])==2*CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xbsunxodgc8L",
    "outputId": "66c82e13-1701-4ca4-932c-eac0f123d0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204043/204043 [16:10<00:00, 210.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 0 = 1549487.308421135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204043/204043 [16:10<00:00, 210.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 1 = 1448604.5771062374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204043/204043 [16:10<00:00, 210.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Epoch 2 = 1426921.0356652737\n",
      "[1549487.308421135, 1448604.5771062374, 1426921.0356652737]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "scifi_model_window5 = CBOW(vocab_size = len(scifi_vocab), embedding_dim=50)\n",
    "\n",
    "# Device for GPU Training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"Training on GPU\")\n",
    "   device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Train\n",
    "train(model = scifi_model_window5, data=scifi_data, vocab=scifi_vocab, device=device, word_to_ix=scifi_word2ix, NUM_EPOCHS=3)\n",
    "torch.save(scifi_model_window5, '/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window5.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5sJ9CpPg7sH"
   },
   "outputs": [],
   "source": [
    "scifi_model_window5_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTMQ9Gnc_mwv"
   },
   "source": [
    "## Part 2 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UpYchOtAfsC",
    "outputId": "585229e2-5648-4d04-8db7-8a77c09d6f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4bS_0Y2QYEL",
    "outputId": "9f6392e3-d4cb-483a-cd92-209136502b2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvVEGJUdhIly",
    "outputId": "7c5439e2-4b08-4ca1-cfb1-74191a471540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "                                              Review  Rating\n",
      "0  fantastic service large hotel caters business ...       5\n",
      "1  great hotel modern hotel good location, locate...       4\n",
      "2  3 star plus glasgowjust got 30th november 4 da...       4\n",
      "3  nice stayed hotel nov 19-23. great little bout...       4\n",
      "4  great place wonderful hotel ideally located me...       5\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "\n",
    "\"\"\"\n",
    "trip_advisor_url = https://drive.google.com/file/d/1foE1JuZJeu5E_4qVge9kExzhvF32teuF/view\n",
    "scifi_url = https://drive.google.com/file/d/13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75/view\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change the paths accordingly\n",
    "# trip_advisor_path = '/content/drive/MyDrive/mlnlp1/exercise-2/data/tripadvisor_hotel_reviews_reduced.csv'\n",
    "# scifi_path = '/content/drive/MyDrive/mlnlp1/exercise-2/data/scifi_reduced.txt'\n",
    "trip_advisor_path = 'tripadvisor_hotel_reviews_reduced.csv'\n",
    "scifi_path = 'scifi_reduced.txt'\n",
    "\n",
    "# DF trip advisor\n",
    "df_trip = pd.read_csv(trip_advisor_path)\n",
    "print(df_trip.head())\n",
    "\n",
    "# Scifi text\n",
    "text_file = open(scifi_path, \"r\")\n",
    "scifi_text = text_file.read()\n",
    "text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4p2kJs2mhZut"
   },
   "outputs": [],
   "source": [
    "# Function to pre-process\n",
    "# 1. Lowercase text\n",
    "# 2. Tokenize based on sentences - split on \".\" - we'll get a list of sentences\n",
    "# 3. For each sentence from 2, tokenize based on punctuations - nltk wordpunct_tokenize - we'll get a list of list of words in a sentence\n",
    "# 4. Remove punctuations\n",
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text_sent_token = text.split(\".\")\n",
    "  text_punct_token = [ wordpunct_tokenize(sent) for sent in text_sent_token]\n",
    "  text_punct_token_cleaned = []\n",
    "  # Remove punctuations from tokenized list of lists.\n",
    "  for txt_list in text_punct_token:\n",
    "    clean_txt = []\n",
    "    for txt in txt_list:\n",
    "      if txt not in string.punctuation:\n",
    "        clean_txt.append(txt)\n",
    "    # Take only texts having more than 1 element\n",
    "    if len(clean_txt)>1:\n",
    "       text_punct_token_cleaned.append(clean_txt)\n",
    "  return text_punct_token_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6LT8VsPNQ5I-"
   },
   "outputs": [],
   "source": [
    "## Required for loading the model\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n",
    "      super(CBOW, self).__init__()\n",
    "      # Embedding layer - Lookup table\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "      # Layer 1 - Since we'll be summing up the context vectors, the input to this layer will be embedding_dim. Output is a 256 dim. vector\n",
    "      self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "      # Adding non-linearity through ReLU\n",
    "      self.relu = nn.ReLU()\n",
    "      # Final layer to get to vocab size dim\n",
    "      self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "      # Log softmax to get probabilities\n",
    "      self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "      # inputs are context vectors. Get embeddings for them\n",
    "      x = self.embeddings(inputs)\n",
    "      # sum all context vectors\n",
    "      x = torch.sum(x,axis=1).view(inputs.shape[0],-1)\n",
    "      #x = sum(x).view(1,-1)\n",
    "      # Add first layer\n",
    "      x = self.linear1(x)\n",
    "      # Add relu\n",
    "      x = self.relu(x)\n",
    "      # Add final layer\n",
    "      x = self.linear2(x)\n",
    "      # Get log softmax\n",
    "      x = self.log_softmax(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9f9fW7fQAgWi"
   },
   "outputs": [],
   "source": [
    "### Load all models\n",
    "trip_model_window2_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window2.pth')\n",
    "trip_model_window5_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/trip_model_window5.pth')\n",
    "scifi_model_window2_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window2.pth')\n",
    "scifi_model_window5_loaded = torch.load('/content/drive/MyDrive/mlnlp1/exercise-2/scifi_model_window5.pth')\n",
    "\n",
    "### Load word2ix dictionaries\n",
    "with open('/content/drive/MyDrive/mlnlp1/exercise-2/trip_word2ix.pickle', 'rb') as handle:\n",
    "    trip_word2ix = pickle.load(handle)\n",
    "\n",
    "with open('/content/drive/MyDrive/mlnlp1/exercise-2/scifi_word2ix.pickle', 'rb') as handle:\n",
    "    scifi_word2ix = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pSoOnlVKE47G"
   },
   "outputs": [],
   "source": [
    "### Function to get pretrained numpy embeddings for each word\n",
    "def get_pretrained_embeddings(model, word2idx):\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "  all_idx = [x for x in range(len(word2idx))]\n",
    "  all_idx = torch.tensor(all_idx, dtype = torch.long, device = device)\n",
    "  pretrained_embeds = model.embeddings(all_idx)\n",
    "  pretrained_embeds = pretrained_embeds.detach().cpu().numpy()\n",
    "\n",
    "  return pretrained_embeds\n",
    "\n",
    "### Function to get idx to word dictionary\n",
    "def get_idx2word(word2idx):\n",
    "  idx2word = dict()\n",
    "  for word, idx in word2idx.items():\n",
    "    idx2word[idx] = word\n",
    "  assert(len(idx2word)==len(word2idx))\n",
    "  return idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfzuz-rMA2y0",
    "outputId": "554352d2-a80c-45d8-d4ee-6d1fb2c347f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trip embeddings: Window = 2 (36894, 50)\n",
      "Shape of trip embeddings: Window = 5 (36894, 50)\n",
      "Shape of scifi embeddings: Window = 2 (111643, 50)\n",
      "Shape of scifi embeddings: Window = 5 (111643, 50)\n"
     ]
    }
   ],
   "source": [
    "## Get idx2word dictionaries and pre-trained embeddings for all models\n",
    "trip_idx2word = get_idx2word(trip_word2ix)\n",
    "trip_embeds_window2 = get_pretrained_embeddings(trip_model_window2_loaded, trip_word2ix)\n",
    "trip_embeds_window5 = get_pretrained_embeddings(trip_model_window5_loaded, trip_word2ix)\n",
    "print(\"Shape of trip embeddings: Window = 2\", trip_embeds_window2.shape)\n",
    "print(\"Shape of trip embeddings: Window = 5\", trip_embeds_window5.shape)\n",
    "\n",
    "scifi_idx2word = get_idx2word(scifi_word2ix)\n",
    "scifi_embeds_window2 = get_pretrained_embeddings(scifi_model_window2_loaded, scifi_word2ix)\n",
    "scifi_embeds_window5 = get_pretrained_embeddings(scifi_model_window5_loaded, scifi_word2ix)\n",
    "print(\"Shape of scifi embeddings: Window = 2\", scifi_embeds_window2.shape)\n",
    "print(\"Shape of scifi embeddings: Window = 5\", scifi_embeds_window5.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oPkx1JdLH91w"
   },
   "outputs": [],
   "source": [
    "## Function to get frequency of words in vocab\n",
    "def get_vocab_freq(raw_llist):\n",
    "  vocab_freq = dict()\n",
    "  for lst in raw_llist:\n",
    "    for el in lst:\n",
    "      if el in vocab_freq:\n",
    "        vocab_freq[el] += 1\n",
    "      else:\n",
    "        vocab_freq[el] = 1\n",
    "  return dict(sorted(vocab_freq.items(), key=lambda kv: kv[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YU9F0VWBOqi-"
   },
   "outputs": [],
   "source": [
    "## Function to get closest words\n",
    "## Closest word should be itself - sanity check\n",
    "## Select closest words having comparable frequency with the input word\n",
    "## vocab_freq stores the frequency of words in vocabulary\n",
    "def get_closest_word(word, word_to_index, index_to_word, emb, vocab_freq, freq_thresh=0.25, topn=11, use_freq=False):\n",
    "  word_distance = []\n",
    "  pdist = nn.PairwiseDistance()\n",
    "  i = word_to_index[word]\n",
    "  v_i = emb[i]\n",
    "  for j in range(len(word_to_index)):\n",
    "    word_j = index_to_word[j]\n",
    "    if use_freq:\n",
    "      if vocab_freq[word_j] >= freq_thresh*vocab_freq[word]:\n",
    "        v_j = emb[j]\n",
    "        word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    else:\n",
    "      v_j = emb[j]\n",
    "      word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "  word_distance.sort(key=lambda x: x[1])\n",
    "  return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFeU_WWmUMIy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxgh-jMwUM3r"
   },
   "source": [
    "#### Testing Trip-Advisor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Na65b-uPQNgd"
   },
   "outputs": [],
   "source": [
    "## Get vocab frequency of trip data\n",
    "\n",
    "# Preprocess\n",
    "trip_pp = df_trip[\"Review\"].apply(lambda x: preprocess(x))\n",
    "\n",
    "# Convert to list of lists\n",
    "trip_pp_llist = []\n",
    "trip_pp_rows = trip_pp.tolist()\n",
    "for trip_pp_row in trip_pp_rows:\n",
    "  for trip_pp_row_list in trip_pp_row:\n",
    "    trip_pp_llist.append(trip_pp_row_list)\n",
    "\n",
    "trip_vocab_freq = get_vocab_freq(trip_pp_llist)\n",
    "\n",
    "\n",
    "## Get vocab frequency of scifi data\n",
    "scifi_preprocessed = preprocess(scifi_text)\n",
    "scifi_vocab_freq = get_vocab_freq(scifi_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3f1gNhlZLoEx"
   },
   "outputs": [],
   "source": [
    "## Get frequency distribution to select 3 nouns, 3 verbs and 3 adjectives\n",
    "\n",
    "## Selecting 3 nouns, verbs and adjectives from trip_vocab_freq\n",
    "trip_nouns = ['hotel','room', 'staff', 'bridge'] # bridge occurs infrequently\n",
    "trip_verbs = ['booked', 'arrived', 'finding'] # finding occurs infrequently\n",
    "trip_adj = ['great','helpful','majestic'] # majestic occurs infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pASSFsrbLyO0",
    "outputId": "3f8146b8-b994-453e-cd73-2193d2cca85c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  hotel\n",
      "Top words using context window 2: \n",
      " [('hotel', 7.071067557262722e-06), ('daythere', 7.1150407791137695), ('fakes', 7.34874153137207), ('antibacterial', 7.442863941192627), ('averge', 7.481241226196289), ('ptns', 7.485942363739014)]\n",
      "Top words using context window 5: \n",
      " [('hotel', 7.071067557262722e-06), ('smirnoff', 6.539915084838867), ('you', 6.664706707000732), ('centeredness', 6.683339595794678), ('tallers', 6.783373832702637), ('frescos', 6.785723686218262)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('hotel', 7.071067557262722e-06), ('nice', 10.170254707336426), ('room', 10.289427757263184), ('not', 10.764053344726562), ('stay', 10.84562873840332), ('great', 11.0220308303833), ('good', 11.04296588897705), ('n', 11.058713912963867), ('just', 11.50808334350586), ('did', 11.63641357421875), ('staff', 11.810798645019531)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('hotel', 7.071067557262722e-06), ('nice', 8.735823631286621), ('room', 9.836873054504395), ('just', 10.294320106506348), ('good', 10.340044975280762), ('did', 10.49057388305664), ('stay', 11.181787490844727), ('staff', 11.305591583251953), ('t', 11.32223892211914), ('not', 11.900873184204102), ('n', 12.151359558105469)]\n",
      "\n",
      "\n",
      "Current Word:  room\n",
      "Top words using context window 2: \n",
      " [('room', 7.071067557262722e-06), ('floormat', 6.93337345123291), ('unhurried', 7.012442111968994), ('combination', 7.124076843261719), ('grilling', 7.15019416809082), ('hostage', 7.19112491607666)]\n",
      "Top words using context window 5: \n",
      " [('room', 7.071067557262722e-06), ('raffles', 6.322586536407471), ('overlookinng', 6.528810024261475), ('transferred', 6.576822757720947), ('creamsickle', 6.5794501304626465), ('quirkiness', 6.616507053375244)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('room', 7.071067557262722e-06), ('clean', 8.496248245239258), ('no', 9.505009651184082), ('good', 9.675029754638672), ('location', 9.854072570800781), ('time', 9.875548362731934), ('day', 9.94225788116455), ('n', 9.94657039642334), ('nice', 9.992109298706055), ('stay', 10.01465129852295), ('rooms', 10.023444175720215)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('room', 7.071067557262722e-06), ('did', 7.560712814331055), ('night', 8.50097370147705), ('clean', 8.888139724731445), ('great', 9.238271713256836), ('rooms', 9.404844284057617), ('staff', 9.443058013916016), ('day', 9.48914909362793), ('breakfast', 9.717561721801758), ('hotel', 9.836875915527344), ('stayed', 9.89282512664795)]\n",
      "\n",
      "\n",
      "Current Word:  staff\n",
      "Top words using context window 2: \n",
      " [('staff', 7.071067557262722e-06), ('varities', 7.880340099334717), ('notmuch', 7.957942008972168), ('dfs', 7.959938049316406), ('3hrs', 8.02213191986084), ('solving', 8.029790878295898)]\n",
      "Top words using context window 5: \n",
      " [('staff', 7.071067557262722e-06), ('atop', 6.731248378753662), ('florentines', 6.867310523986816), ('scarmbled', 7.074032306671143), ('milford', 7.080294609069824), ('890pp', 7.098931789398193)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('staff', 7.071067557262722e-06), ('restaurants', 8.979288101196289), ('the', 9.25534725189209), ('city', 9.919352531433105), ('day', 9.988219261169434), ('time', 10.19427490234375), ('trip', 10.341984748840332), ('3', 10.42031192779541), ('recommend', 10.479532241821289), ('2', 10.49714183807373), ('hotels', 10.500456809997559)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('staff', 7.071067557262722e-06), ('place', 7.838854789733887), ('time', 8.169486999511719), ('better', 8.704246520996094), ('floor', 8.705599784851074), ('hotels', 8.805439949035645), ('day', 8.922090530395508), ('bed', 9.203536987304688), ('rooms', 9.268948554992676), ('bar', 9.306385040283203), ('city', 9.428123474121094)]\n",
      "\n",
      "\n",
      "Current Word:  bridge\n",
      "Top words using context window 2: \n",
      " [('bridge', 7.071067557262722e-06), ('bar10', 6.657200336456299), ('hustled', 6.876956939697266), ('congenial', 7.13400411605835), ('combination', 7.228611946105957), ('fielded', 7.438404560089111)]\n",
      "Top words using context window 5: \n",
      " [('bridge', 7.071067557262722e-06), ('budgeting', 5.846224308013916), ('persistantly', 6.044887065887451), ('seating', 6.156558036804199), ('forecourt', 6.392253398895264), ('tania', 6.393515586853027)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('bridge', 7.071067557262722e-06), ('combination', 7.228611946105957), ('typically', 7.500279903411865), ('faced', 7.835662841796875), ('ran', 8.089554786682129), ('corridor', 8.09927749633789), ('transfer', 8.147374153137207), ('immaculate', 8.158283233642578), ('extra', 8.18408203125), ('embassy', 8.229692459106445), ('overpriced', 8.242166519165039)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('bridge', 7.071067557262722e-06), ('seating', 6.156558036804199), ('steep', 6.466039180755615), ('place', 6.714748382568359), ('whilst', 6.7360968589782715), ('gran', 6.860733509063721), ('far', 7.073087692260742), ('completely', 7.085731029510498), ('flying', 7.089111328125), ('very', 7.093839168548584), ('casa', 7.124594211578369)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top n closest words for nouns\n",
    "for noun in trip_nouns:\n",
    "  print(\"Current Word: \", noun)\n",
    "  topn_closest_words_window2 = get_closest_word(noun, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(noun, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(noun, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(noun, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acBjqABRPyzo",
    "outputId": "2f6068db-2b0a-4555-fe4e-6e0db8cd57d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  booked\n",
      "Top words using context window 2: \n",
      " [('booked', 7.071067557262722e-06), ('degrading', 7.154239177703857), ('towards', 7.2299885749816895), ('bordeux', 7.316226959228516), ('theregood', 7.339205741882324), ('thorogh', 7.373607635498047)]\n",
      "Top words using context window 5: \n",
      " [('booked', 7.071067557262722e-06), ('mil', 7.586578369140625), ('breeding', 7.60408353805542), ('peaked', 8.02645206451416), ('becausewe', 8.053140640258789), ('soilded', 8.111804008483887)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('booked', 7.071067557262722e-06), ('business', 8.284531593322754), ('book', 8.403924942016602), ('restaurants', 8.440139770507812), ('prices', 8.480257987976074), ('bring', 8.5995454788208), ('yes', 8.767535209655762), ('large', 8.84104061126709), ('2', 8.878698348999023), ('convenient', 8.880636215209961), ('line', 8.884795188903809)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('booked', 7.071067557262722e-06), ('use', 8.752636909484863), ('pretty', 8.753384590148926), ('mini', 8.82991886138916), ('business', 9.094027519226074), ('place', 9.14390754699707), ('far', 9.149474143981934), ('bring', 9.205549240112305), ('worth', 9.226174354553223), ('said', 9.370829582214355), ('expected', 9.39095687866211)]\n",
      "\n",
      "\n",
      "Current Word:  arrived\n",
      "Top words using context window 2: \n",
      " [('arrived', 7.071067557262722e-06), ('alll', 5.957443714141846), ('graduates', 6.160930156707764), ('friendley', 6.199235439300537), ('overseeing', 6.221738815307617), ('firewood', 6.25324010848999)]\n",
      "Top words using context window 5: \n",
      " [('arrived', 7.071067557262722e-06), ('pescatore', 6.759324073791504), ('garrett', 6.782865524291992), ('stuffthe', 6.895144939422607), ('300k', 6.907783031463623), ('rennaissance', 6.915228843688965)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('arrived', 7.071067557262722e-06), ('corner', 6.769083499908447), ('plaza', 6.826190948486328), ('use', 6.9361162185668945), ('comfortable', 7.015511989593506), ('taxi', 7.123593330383301), ('choice', 7.194443702697754), ('english', 7.203039646148682), ('convenient', 7.322325706481934), ('reasonable', 7.379371166229248), ('try', 7.414639472961426)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('arrived', 7.071067557262722e-06), ('nearby', 7.786351680755615), ('loved', 7.843232154846191), ('located', 7.94815731048584), ('dont', 8.037534713745117), ('like', 8.20236587524414), ('ocean', 8.226262092590332), ('outside', 8.256152153015137), ('bar', 8.272931098937988), ('request', 8.287704467773438), ('overall', 8.299460411071777)]\n",
      "\n",
      "\n",
      "Current Word:  finding\n",
      "Top words using context window 2: \n",
      " [('finding', 7.071067557262722e-06), ('earth', 5.676231384277344), ('perceive', 5.710001468658447), ('amazedat', 5.923399448394775), ('task', 5.946594715118408), ('24th', 6.017587184906006)]\n",
      "Top words using context window 5: \n",
      " [('finding', 7.071067557262722e-06), ('chocalate', 5.962070941925049), ('suprized', 6.2966437339782715), ('somone', 6.303151607513428), ('community', 6.314207553863525), ('nevis', 6.325850009918213)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('finding', 7.071067557262722e-06), ('switched', 6.3585357666015625), ('e', 6.436366558074951), ('presented', 6.49741268157959), ('120', 6.5190582275390625), ('arrival', 6.640464782714844), ('classic', 6.702696323394775), ('unable', 6.7085418701171875), ('reported', 6.727651596069336), ('control', 6.799676895141602), ('writing', 6.805020809173584)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('finding', 7.071067557262722e-06), ('basket', 6.3635358810424805), ('them', 6.857925891876221), ('china', 6.955080986022949), ('tried', 6.996697902679443), ('wharf', 7.028076648712158), ('right', 7.051466941833496), ('kuta', 7.108212947845459), ('enormous', 7.110820770263672), ('friday', 7.163728713989258), ('larger', 7.185604572296143)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top n closest words for verbs\n",
    "for verb in trip_verbs:\n",
    "  print(\"Current Word: \", verb)\n",
    "  topn_closest_words_window2 = get_closest_word(verb, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(verb, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(verb, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(verb, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rz2_qea8TfCs",
    "outputId": "3f437e46-e099-4d8c-b20d-364fcb4845f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  great\n",
      "Top words using context window 2: \n",
      " [('great', 7.071067557262722e-06), ('sooooooo', 7.483932018280029), ('maintainence', 7.635804653167725), ('substituted', 7.64486837387085), ('corner', 7.653206825256348), ('dispassionate', 7.690426826477051)]\n",
      "Top words using context window 5: \n",
      " [('great', 7.071067557262722e-06), ('tore', 7.171984672546387), ('detour', 7.46504020690918), ('harmful', 7.708715438842773), ('lagoon', 7.713411808013916), ('scubaed', 7.7191596031188965)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('great', 7.071067557262722e-06), ('stay', 9.406214714050293), ('helpful', 9.57632064819336), ('clean', 9.777229309082031), ('time', 9.860251426696777), ('nice', 9.872404098510742), ('night', 9.90973949432373), ('people', 9.919472694396973), ('best', 10.043133735656738), ('beach', 10.0620698928833), ('small', 10.094267845153809)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('great', 7.071067557262722e-06), ('place', 9.210975646972656), ('room', 9.238271713256836), ('bar', 9.304732322692871), ('rooms', 9.689080238342285), ('did', 10.17997932434082), ('2', 10.24163818359375), ('food', 10.318475723266602), ('night', 10.377157211303711), ('friendly', 10.418194770812988), ('clean', 10.426919937133789)]\n",
      "\n",
      "\n",
      "Current Word:  helpful\n",
      "Top words using context window 2: \n",
      " [('helpful', 7.071067557262722e-06), ('modeled', 6.840415000915527), ('noise', 6.88803243637085), ('batur', 6.9412922859191895), ('witch', 6.947271823883057), ('undervalued', 7.142728328704834)]\n",
      "Top words using context window 5: \n",
      " [('helpful', 7.071067557262722e-06), ('discernign', 6.611217975616455), ('vith', 6.644071102142334), ('chose', 6.768656253814697), ('routines', 6.921473026275635), ('piano', 6.981733798980713)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('helpful', 7.071067557262722e-06), ('noise', 6.88803243637085), ('things', 7.404886245727539), ('7', 7.871766090393066), ('time', 8.146087646484375), ('drinks', 8.146485328674316), ('old', 8.192484855651855), ('standard', 8.25359058380127), ('called', 8.254939079284668), ('choice', 8.294174194335938), ('drink', 8.36839771270752)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('helpful', 7.071067557262722e-06), ('dinner', 7.68801212310791), ('left', 7.747203350067139), ('business', 8.08238697052002), ('felt', 8.199460983276367), ('park', 8.2885103225708), ('night', 8.399307250976562), ('rate', 8.419997215270996), ('make', 8.489202499389648), ('2', 8.542309761047363), ('choice', 8.616579055786133)]\n",
      "\n",
      "\n",
      "Current Word:  majestic\n",
      "Top words using context window 2: \n",
      " [('majestic', 7.071067557262722e-06), ('toerism', 6.6138386726379395), ('hours', 6.7503437995910645), ('encouraged', 6.823374271392822), ('herman', 6.889046669006348), ('barcelona__ç_é_', 6.930231094360352)]\n",
      "Top words using context window 5: \n",
      " [('majestic', 7.071067557262722e-06), ('chic', 6.14655065536499), ('phony', 6.378418922424316), ('raffles', 6.38958215713501), ('nestor', 6.491503715515137), ('loudly', 6.527940273284912)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('majestic', 7.071067557262722e-06), ('hours', 6.7503437995910645), ('rate', 7.222047328948975), ('sunday', 7.403398513793945), ('rates', 7.501216888427734), ('golf', 7.501349449157715), ('mind', 7.6752777099609375), ('anniversary', 7.681216239929199), ('tv', 7.68575382232666), ('sydney', 7.6990156173706055), ('smile', 7.703586101531982)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('majestic', 7.071067557262722e-06), ('chic', 6.14655065536499), ('clubs', 6.724300384521484), ('mini', 6.727129936218262), ('past', 6.761188983917236), ('kitchen', 6.836026668548584), ('disappointing', 6.867189884185791), ('driving', 6.8844218254089355), ('pretty', 6.921935558319092), ('hop', 6.9703369140625), ('hit', 7.058217525482178)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top closes words for adjectives\n",
    "for adj in trip_adj:\n",
    "  print(\"Current Word: \", adj)\n",
    "  topn_closest_words_window2 = get_closest_word(adj, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(adj, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(adj, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(adj, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5), trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzLm-hqeUa3C"
   },
   "source": [
    "#### Testing Scifi Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z4eX4cy4T71U"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Selecting 3 nouns, verbs and adjectives from trip_vocab_freq\n",
    "scifi_nouns = ['man', 'eyes', 'president'] # president occurs infrequently\n",
    "scifi_verbs = ['said', 'looked', 'eat'] # eat occurs infrequently\n",
    "scifi_adj = ['good', 'old', 'poor'] # poor occurs infrequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwGMZfbBVYnV",
    "outputId": "b85f15f0-8896-4319-b781-54f59b5b8d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  man\n",
      "Top words using context window 2: \n",
      " [('man', 7.071067557262722e-06), ('ytccuuough', 6.1966753005981445), ('loiew', 6.429879665374756), ('ofmush', 6.569863796234131), ('straps', 6.574796676635742), ('helmet', 6.598606109619141)]\n",
      "Top words using context window 5: \n",
      " [('man', 7.071067557262722e-06), ('hejd', 6.230587959289551), ('headstart', 6.558587551116943), ('matriarch', 6.641236305236816), ('looung', 6.741540908813477), ('triumphantly', 6.791837692260742)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('man', 7.071067557262722e-06), ('without', 7.140392780303955), ('still', 7.851052284240723), ('where', 8.03598690032959), ('another', 8.102642059326172), ('mind', 8.128649711608887), ('first', 8.157269477844238), ('no', 8.287017822265625), ('again', 8.295594215393066), ('my', 8.33771800994873), ('let', 8.364876747131348)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('man', 7.071067557262722e-06), ('around', 8.008111000061035), ('about', 8.196211814880371), ('there', 8.472517967224121), ('great', 8.474035263061523), ('off', 8.475293159484863), ('he', 8.519294738769531), ('not', 8.528158187866211), (',\"', 8.577947616577148), ('most', 8.590346336364746), ('space', 8.606943130493164)]\n",
      "\n",
      "\n",
      "Current Word:  eyes\n",
      "Top words using context window 2: \n",
      " [('eyes', 7.071067557262722e-06), ('andblack', 7.40976619720459), ('auschwitz', 7.82407808303833), ('hellstorm', 7.826470851898193), ('tocs', 7.831721782684326), ('sting', 7.846226692199707)]\n",
      "Top words using context window 5: \n",
      " [('eyes', 7.071067557262722e-06), ('mati', 6.687601089477539), ('felony', 6.7717604637146), ('skylarking', 6.791901588439941), ('rookies', 6.81085729598999), ('ariel', 6.82222843170166)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('eyes', 7.071067557262722e-06), ('about', 8.90227222442627), ('even', 9.027610778808594), ('so', 9.125539779663086), ('both', 9.175600051879883), ('what', 9.187559127807617), ('point', 9.23692798614502), ('mind', 9.496200561523438), ('held', 9.54585075378418), ('happened', 9.614137649536133), ('must', 9.627249717712402)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('eyes', 7.071067557262722e-06), ('used', 7.197762966156006), ('read', 7.667297840118408), ('five', 7.779964923858643), ('almost', 7.835725784301758), ('control', 7.858102321624756), ('really', 7.880575656890869), ('above', 7.89900541305542), ('same', 8.171777725219727), ('way', 8.374471664428711), ('my', 8.375723838806152)]\n",
      "\n",
      "\n",
      "Current Word:  president\n",
      "Top words using context window 2: \n",
      " [('president', 7.071067557262722e-06), ('unremarkably', 7.015797138214111), ('ncy', 7.046952247619629), ('rarer', 7.151254177093506), ('hipster', 7.204301357269287), ('vation', 7.245964050292969)]\n",
      "Top words using context window 5: \n",
      " [('president', 7.071067557262722e-06), ('ginwidlah', 5.592962741851807), ('kish', 5.781639575958252), ('tarried', 6.129746913909912), ('annex', 6.213682174682617), ('arizona', 6.226224899291992)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('president', 7.071067557262722e-06), ('rogue', 7.717016696929932), ('spent', 8.08859634399414), ('tapes', 8.171854019165039), ('gree', 8.28266716003418), ('next', 8.284293174743652), ('since', 8.332175254821777), ('tiny', 8.379899978637695), ('blaster', 8.404074668884277), ('trembling', 8.432272911071777), ('catch', 8.438361167907715)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('president', 7.071067557262722e-06), ('pieces', 6.309770584106445), ('brass', 6.676476001739502), ('station', 6.8893046379089355), ('her', 7.02464485168457), ('dragged', 7.059435844421387), ('counted', 7.067307472229004), ('certain', 7.069771766662598), ('log', 7.126338005065918), ('editor', 7.133800983428955), ('masters', 7.147698402404785)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top n closest words for nouns\n",
    "for noun in scifi_nouns:\n",
    "  print(\"Current Word: \", noun)\n",
    "  topn_closest_words_window2 = get_closest_word(noun, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(noun, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(noun, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(noun, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjxw7BYOXim4",
    "outputId": "b0026dd3-dbfb-487a-f2f7-176970ae5164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  said\n",
      "Top words using context window 2: \n",
      " [('said', 7.071067557262722e-06), ('psychopathological', 7.161562442779541), ('teleportations', 7.164734363555908), ('recordsi', 7.181768417358398), ('walkdown', 7.1959943771362305), ('tepeni', 7.211658000946045)]\n",
      "Top words using context window 5: \n",
      " [('said', 7.071067557262722e-06), ('drfflif', 6.66356086730957), ('urigiaaf', 6.882411956787109), ('archaeologists', 6.93709135055542), ('beluthahatchie', 6.970693111419678), ('jmfe', 7.1472649574279785)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('said', 7.071067557262722e-06), ('this', 8.031986236572266), ('here', 8.151091575622559), ('through', 8.893613815307617), ('s', 8.981287956237793), ('?\"', 9.002523422241211), ('it', 9.012069702148438), ('even', 9.063409805297852), ('how', 9.14681625366211), ('so', 9.1686429977417), ('are', 9.196456909179688)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('said', 7.071067557262722e-06), ('don', 8.41417407989502), ('?\"', 8.429239273071289), ('his', 8.611624717712402), ('about', 8.649161338806152), ('an', 8.866805076599121), ('us', 8.909385681152344), ('way', 8.982842445373535), ('off', 9.012895584106445), ('man', 9.107386589050293), ('its', 9.206231117248535)]\n",
      "\n",
      "\n",
      "Current Word:  looked\n",
      "Top words using context window 2: \n",
      " [('looked', 7.071067557262722e-06), ('vesix', 6.411952018737793), ('danny', 6.457793712615967), ('ecstatic', 6.717048168182373), ('graypec', 6.757779121398926), ('mowers', 6.836764812469482)]\n",
      "Top words using context window 5: \n",
      " [('looked', 7.071067557262722e-06), ('sphere', 6.149446487426758), ('anthropology', 6.254521369934082), ('otherness', 6.2786688804626465), ('superintend', 6.3091278076171875), ('regenerative', 6.318289279937744)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('looked', 7.071067557262722e-06), ('almost', 7.765730381011963), ('d', 7.8029584884643555), ('this', 8.023635864257812), ('science', 8.02464771270752), ('work', 8.350424766540527), ('held', 8.419989585876465), ('keep', 8.426718711853027), ('every', 8.431392669677734), ('told', 8.432861328125), ('isn', 8.468539237976074)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('looked', 7.071067557262722e-06), ('not', 7.372743606567383), ('part', 7.390920639038086), ('fiction', 7.400472164154053), ('wanted', 7.636682033538818), ('called', 7.655518531799316), ('short', 7.665722370147705), ('where', 7.706684112548828), ('next', 7.906850814819336), ('small', 7.9077863693237305), ('some', 7.937670707702637)]\n",
      "\n",
      "\n",
      "Current Word:  eat\n",
      "Top words using context window 2: \n",
      " [('eat', 7.071067557262722e-06), ('maghelp', 6.4119744300842285), ('himwas', 6.604079246520996), ('mechounits', 6.608515739440918), ('belches', 6.66560173034668), ('comrades', 6.681324481964111)]\n",
      "Top words using context window 5: \n",
      " [('eat', 7.071067557262722e-06), ('klars', 6.212573051452637), ('swallow', 6.368799686431885), ('justin', 6.378588676452637), ('comeback', 6.490656852722168), ('ostentatiously', 6.541658878326416)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('eat', 7.071067557262722e-06), ('landed', 7.189903259277344), ('b', 7.217710494995117), ('quiet', 7.221595764160156), ('glass', 7.241941452026367), ('attacked', 7.325897216796875), ('consider', 7.392586708068848), ('tour', 7.411027908325195), ('gear', 7.4453959465026855), ('directions', 7.453084945678711), ('rex', 7.47431755065918)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('eat', 7.071067557262722e-06), ('capable', 6.859894752502441), ('never', 7.082798480987549), ('wrote', 7.2059526443481445), ('battle', 7.241952419281006), ('interesting', 7.243092060089111), ('st', 7.26455545425415), ('interested', 7.310184955596924), ('security', 7.35705041885376), ('eyebrows', 7.398166179656982), ('postage', 7.403764247894287)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top n closes words for verbs\n",
    "for verb in scifi_verbs:\n",
    "  print(\"Current Word: \", verb)\n",
    "  topn_closest_words_window2 = get_closest_word(verb, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(verb, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(verb, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(verb, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH1qNtyEX6nU",
    "outputId": "1b4361bb-768b-47cb-cd18-e6f0dada3e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  good\n",
      "Top words using context window 2: \n",
      " [('good', 7.071067557262722e-06), ('skunki', 6.017290115356445), ('philosopher', 6.096534252166748), ('transfusion', 6.340238571166992), ('pfestige', 6.363999843597412), ('thirtyfifth', 6.375101566314697)]\n",
      "Top words using context window 5: \n",
      " [('good', 7.071067557262722e-06), ('sfmagazine', 6.839204788208008), ('hurrah', 6.955796718597412), ('eightyfive', 7.0804643630981445), ('emulation', 7.096042156219482), ('whisper', 7.142617702484131)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('good', 7.071067557262722e-06), ('feel', 6.788490295410156), ('except', 7.207240104675293), ('something', 7.243300437927246), ('began', 7.376669406890869), ('yet', 7.458994388580322), ('hours', 7.4830002784729), ('going', 7.495906352996826), ('open', 7.546849250793457), ('at', 7.628347396850586), ('got', 7.764412879943848)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('good', 7.071067557262722e-06), ('around', 8.333001136779785), ('years', 8.438902854919434), ('both', 8.4570951461792), ('more', 8.59307861328125), ('any', 8.647302627563477), ('should', 8.790623664855957), ('a', 8.826746940612793), ('way', 8.959698677062988), ('city', 9.068593978881836), ('others', 9.06970500946045)]\n",
      "\n",
      "\n",
      "Current Word:  old\n",
      "Top words using context window 2: \n",
      " [('old', 7.071067557262722e-06), ('tetraploid', 6.320662021636963), ('grittylooking', 6.364109992980957), ('kited', 6.411200046539307), ('puglike', 6.45253849029541), ('inflation', 6.505138874053955)]\n",
      "Top words using context window 5: \n",
      " [('old', 7.071067557262722e-06), ('toggle', 6.660755157470703), ('affrays', 6.699178218841553), ('tater', 6.788771152496338), ('drillings', 6.815727710723877), ('rehearse', 6.82301664352417)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('old', 7.071067557262722e-06), ('keep', 7.413689613342285), ('might', 7.558547496795654), ('this', 7.638154029846191), ('s', 7.833471775054932), ('off', 7.964145183563232), ('get', 7.988694190979004), ('sat', 8.005304336547852), ('didn', 8.051188468933105), ('die', 8.07496452331543), ('walked', 8.093829154968262)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('old', 7.071067557262722e-06), ('number', 7.64933443069458), ('beyond', 7.713184356689453), ('not', 7.737045764923096), ('fact', 7.816811561584473), ('found', 7.857144832611084), ('you', 7.9804182052612305), ('became', 8.145150184631348), ('ten', 8.175712585449219), ('light', 8.235340118408203), ('most', 8.312362670898438)]\n",
      "\n",
      "\n",
      "Current Word:  poor\n",
      "Top words using context window 2: \n",
      " [('poor', 7.071067557262722e-06), ('eor', 6.418169021606445), ('carbines', 6.456538200378418), ('highshouldered', 6.485654354095459), ('foraging', 6.579033374786377), ('lungsful', 6.644907474517822)]\n",
      "Top words using context window 5: \n",
      " [('poor', 7.071067557262722e-06), ('blundy', 6.479696750640869), ('freddi', 6.595727920532227), ('gordinis', 6.622042179107666), ('chemistical', 6.776627540588379), ('frog', 6.868781566619873)]\n",
      "\n",
      "\n",
      "================== Using Frequency Thresholds =====================\n",
      "Top words using frequency thresholds - context window = 2: \n",
      " [('poor', 7.071067557262722e-06), ('atomic', 7.25370454788208), ('whisper', 7.261146545410156), ('bars', 7.261996746063232), ('color', 7.268069267272949), ('loaded', 7.2709832191467285), ('throat', 7.2874932289123535), ('sheer', 7.354554653167725), ('reaching', 7.376532077789307), ('faded', 7.392841339111328), ('cold', 7.444390773773193)]\n",
      "Top words using frequency thresholds - context window = 5: \n",
      " [('poor', 7.071067557262722e-06), ('instrument', 7.0268683433532715), ('neat', 7.160772800445557), ('shape', 7.319815158843994), ('political', 7.437619686126709), ('lonely', 7.45481014251709), ('later', 7.522428512573242), ('ahead', 7.538618564605713), ('doesn', 7.539554119110107), ('blind', 7.564450740814209), ('peculiar', 7.6827874183654785)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top n closes words for adjectives\n",
    "for adj in scifi_adj:\n",
    "  print(\"Current Word: \", adj)\n",
    "  topn_closest_words_window2 = get_closest_word(adj, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(adj, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, topn=6)\n",
    "  print(\"Top words using context window 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"================== Using Frequency Thresholds =====================\")\n",
    "  topn_closest_words_window2 = get_closest_word(adj, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 2: \\n\", topn_closest_words_window2)\n",
    "  topn_closest_words_window5 = get_closest_word(adj, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5), scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using frequency thresholds - context window = 5: \\n\", topn_closest_words_window5)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya4zX1OuYuCZ"
   },
   "source": [
    "#### Common words in both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tuHVYwDYOqy",
    "outputId": "05e193e6-3a93-404e-d749-a9b8307884b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word:  festive\n",
      "Top words using context window 2 on Sci-fi Dataset: \n",
      " [('festive', 7.071067557262722e-06), ('f', 6.074956893920898), ('macnessa', 6.1809821128845215), ('waxen', 6.191169261932373), ('dreadnaughts', 6.225739479064941), ('somethink', 6.276749610900879), ('drivel', 6.383573055267334), ('lifelessness', 6.413447380065918), ('corbacco', 6.429171085357666), ('structures', 6.483883380889893), ('rex', 6.485265731811523)]\n",
      "Top words using context window 2 on Trip Advisor: \n",
      " [('festive', 7.071067557262722e-06), ('itchy', 5.819334983825684), ('briefing', 5.927561283111572), ('argumentative', 6.133767127990723), ('definelty', 6.149537563323975), ('infallible', 6.186091899871826), ('brains', 6.272236347198486), ('swabs', 6.290041446685791), ('venezuela', 6.337795734405518), ('loaction', 6.351245403289795), ('gang', 6.364889144897461)]\n",
      "Top words using context window 5 on Sci-fi Dataset: \n",
      " [('festive', 7.071067557262722e-06), ('sluiced', 5.435795783996582), ('microcircuits', 5.556827068328857), ('collaborating', 5.584251880645752), ('commended', 5.620745658874512), ('pauli', 5.718577861785889), ('impromptu', 5.723856449127197), ('saving', 5.743218898773193), ('peri', 5.783431529998779), ('williamson', 5.8645148277282715), ('thermos', 5.8667802810668945)]\n",
      "Top words using context window 5 on Trip Advisor: \n",
      " [('festive', 7.071067557262722e-06), ('dismayed', 6.091696262359619), ('shinkansen', 6.3713765144348145), ('hoards', 6.47831392288208), ('minding', 6.507296562194824), ('critiques', 6.674073219299316), ('hosted', 6.683778285980225), ('accessories', 6.703507900238037), ('mileage', 6.731977462768555), ('cuff', 6.7693257331848145), ('aerobus', 6.802340507507324)]\n",
      "\n",
      "\n",
      "Current Word:  incredible\n",
      "Top words using context window 2 on Sci-fi Dataset: \n",
      " [('incredible', 7.071067557262722e-06), ('cone', 6.858514308929443), ('hue', 6.895654678344727), ('gallery', 6.964757919311523), ('alan', 6.9902448654174805), ('hawkins', 7.104575157165527), ('culture', 7.131253719329834), ('eager', 7.191163063049316), ('puzzles', 7.274726390838623), ('above', 7.305678844451904), ('faculties', 7.319045543670654)]\n",
      "Top words using context window 2 on Trip Advisor: \n",
      " [('incredible', 7.071067557262722e-06), ('round', 7.2102179527282715), ('change', 7.250071048736572), ('pillow', 7.298600673675537), ('properties', 7.4671549797058105), ('paid', 7.471174240112305), ('don__ç_é_', 7.475256443023682), ('considered', 7.610840320587158), ('priced', 7.670408725738525), ('inviting', 7.70996618270874), ('st', 7.723106384277344)]\n",
      "Top words using context window 5 on Sci-fi Dataset: \n",
      " [('incredible', 7.071067557262722e-06), ('plate', 6.564102649688721), ('absorbed', 6.774073123931885), ('web', 6.8795928955078125), ('mustache', 6.996598243713379), ('depend', 7.042361259460449), ('middle', 7.0430908203125), ('schools', 7.118149757385254), ('capable', 7.132270336151123), ('humanity', 7.149388313293457), ('editor', 7.183242321014404)]\n",
      "Top words using context window 5 on Trip Advisor: \n",
      " [('incredible', 7.071067557262722e-06), ('nicest', 7.612539291381836), ('affinia', 7.613874912261963), ('place', 8.05701732635498), ('ultra', 8.10787582397461), ('feb', 8.142895698547363), ('return', 8.3043212890625), ('flying', 8.351126670837402), ('florence', 8.404332160949707), ('sense', 8.408732414245605), ('finally', 8.41638469696045)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get common vocab\n",
    "#common_vocab = trip_vocab.intersection(scifi_vocab)\n",
    "common_words = [\"festive\",\"incredible\"]\n",
    "\n",
    "for word in common_words:\n",
    "  print(\"Current Word: \", word)\n",
    "  top5_scifi_window2 = get_closest_word(word, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window2),\n",
    "                                                scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using context window 2 on Sci-fi Dataset: \\n\", top5_scifi_window2)\n",
    "\n",
    "  top5_hotel_window2 = get_closest_word(word, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window2),\n",
    "                                                trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using context window 2 on Trip Advisor: \\n\", top5_hotel_window2)\n",
    "\n",
    "  top5_scifi_window5 = get_closest_word(word, scifi_word2ix, scifi_idx2word,\n",
    "                                                torch.tensor(scifi_embeds_window5),\n",
    "                                                scifi_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using context window 5 on Sci-fi Dataset: \\n\", top5_scifi_window5)\n",
    "\n",
    "\n",
    "  top5_hotel_window5 = get_closest_word(word, trip_word2ix, trip_idx2word,\n",
    "                                                torch.tensor(trip_embeds_window5),\n",
    "                                                trip_vocab_freq, use_freq=True)\n",
    "  print(\"Top words using context window 5 on Trip Advisor: \\n\", top5_hotel_window5)\n",
    "\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UmBte-gY71t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
