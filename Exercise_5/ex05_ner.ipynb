{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CtjtNkersgL",
        "outputId": "618551a0-0b93-4f86-e5e2-2dd301b0153d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "6bsheYiGsedt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading and Exploration\n",
        "\n",
        "In this section, we will first load the dataset, generate two datasets for training, then explore the dataset.\n",
        "\n",
        "We choose Hindi language. Since it's not english, have a Hugging Face BERT-base model, and it the dataset contains at least 7000 sentences, therefore it's a valid choice of language.\n",
        "\n",
        "We created 2 datasets:\n",
        "1. dataset_1 with 1000 sentences\n",
        "2. dataset_2 with 3000 sentences"
      ],
      "metadata": {
        "id": "eXB_dGgA1H7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download ner dataset\n",
        "# chosen language: hindi\n",
        "full_dataset = load_dataset(\"polyglot_ner\", \"hi\")"
      ],
      "metadata": {
        "id": "HQPDz0_wsU9K"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCANFAVrzVvQ",
        "outputId": "da4da05c-eff6-441b-9c1f-ab0b52a0e47e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if chosen dataset is a valid dataset\n",
        "# Hindi is not English : therefore Valid\n",
        "# Hugging Face BERT-base model for the language exist (bhavikardeshna/multilingual-bert-base-cased-hindi): therefore Valid\n",
        "# The dataset contains at least 7000 sentences : therefore Valid\n",
        "len(full_dataset[\"train\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2vcj02OsjVy",
        "outputId": "da3b95c5-9111-4bd4-e14a-d7c7dda94fac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401648"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT 2 DATASETS FOR TRAINING, 1 FOR EVALUATION\n",
        "\n",
        "# extract dataset 1 with 1000 sentences\n",
        "train_dataset_1 = full_dataset[\"train\"].shuffle(seed=42).select([i for i in range(1000)])\n",
        "print(len(train_dataset_1))\n",
        "# extract dataset 2 with 3000 sentences\n",
        "train_dataset_2 = full_dataset[\"train\"].shuffle(seed=42).select([i for i in range(3000)])\n",
        "print(len(train_dataset_2))\n",
        "# an evaluation dataset\n",
        "eval_dataset_1 = full_dataset[\"train\"].shuffle(seed=42).select([i for i in range(3000, 5000)])\n",
        "print(len(eval_dataset_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YsWAs3VzEY0",
        "outputId": "2a8c8039-9c7d-477b-ed20-08d25cf1d6aa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "3000\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIrwMvAFzZRC",
        "outputId": "5147eb87-4958-4f10-b202-1ce4524b22e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['id', 'lang', 'words', 'ner'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1[0:2][\"words\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9o10sE90ppN",
        "outputId": "6416abbf-8837-4a4f-ca8c-c0cdacd0c007"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ये', 'मेक्सिको', 'राष्ट्र', 'से', 'थे', '।'],\n",
              " ['2006',\n",
              "  'में',\n",
              "  ',',\n",
              "  'किडमैन',\n",
              "  'को',\n",
              "  'ऑस्ट्रेलिया',\n",
              "  'के',\n",
              "  'सर्वोच्च',\n",
              "  'नागरिक',\n",
              "  'सम्मान',\n",
              "  'कम्पानियन',\n",
              "  'ऑफ़',\n",
              "  'द',\n",
              "  'ऑर्डर',\n",
              "  'ऑफ़',\n",
              "  'ऑस्ट्रेलिया',\n",
              "  'से',\n",
              "  'नवाज़ा',\n",
              "  'गया',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1[0:2][\"ner\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGa81fuv0vuo",
        "outputId": "ceaa8b06-c567-4aac-ec5f-77b87220c268"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['O', 'LOC', 'O', 'O', 'O', 'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'PER',\n",
              "  'O',\n",
              "  'LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = [label for sentence_labels in full_dataset[\"train\"][\"ner\"] for label in sentence_labels]\n",
        "unique_labels = set(all_labels)\n",
        "print(unique_labels)\n",
        "label2id = {k: v for v, k in enumerate(unique_labels)}\n",
        "print(label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Q-yd-DWQHK",
        "outputId": "fd1e505f-51ee-470c-ea02-c27024deedf7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LOC', 'ORG', 'PER', 'O'}\n",
            "{'LOC': 0, 'ORG': 1, 'PER': 2, 'O': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization and important functions\n",
        "\n",
        "In this section, we will initialize several things and define all the important functions we wanna use for this assignment.\n",
        "1. Initialize tokenizer and bert-base model\n",
        "2. Define custom dataset which retuns input ids, attention masks, labels of the dataset\n",
        "3. Defining dataloaders\n",
        "4. Training function\n",
        "5. Evaluation function\n"
      ],
      "metadata": {
        "id": "DYrKB2LV7tuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained Hugging Face BERT-base model for multilingual languages\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "model = BertForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")"
      ],
      "metadata": {
        "id": "OaxaIeiM16_o"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE CUSTOM DATASET WHICH RETURNS INPUT_IDS, ATTENTION_MASKS, AND LABELS OF THE DATASET\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, texts, labels):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.label_encoder = LabelEncoder()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    # step 1: tokenize (and adapt corresponding labels)\n",
        "    for word, label in zip(self.texts[idx], self.labels[idx]):\n",
        "      # Tokenize the word and count # of subwords the word is broken into\n",
        "\n",
        "      tokenized_word = tokenizer.tokenize(word)\n",
        "      n_subwords = len(tokenized_word)\n",
        "\n",
        "      # Add the tokenized word to the final tokenized word list\n",
        "      tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "      # Add the same label to the new list of labels `n_subwords` times\n",
        "      labels.extend([label] * n_subwords)\n",
        "\n",
        "    # step 2: add special tokens (and corresponding labels)\n",
        "    tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "    labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "    labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "    # step 3: truncating/padding\n",
        "    maxlen = 128\n",
        "\n",
        "    if (len(tokenized_sentence) > maxlen):\n",
        "      # truncate\n",
        "      tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "      labels = labels[:maxlen]\n",
        "    else:\n",
        "      # pad\n",
        "      tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "      labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "    # step 4: obtain the attention mask\n",
        "    attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "    # step 5: convert tokens to input ids\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "    label_ids = [label2id[label] for label in labels]\n",
        "    return {\n",
        "          'ids': torch.tensor(ids, dtype=torch.long),\n",
        "          'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "          'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "kApUYOJK52Ei"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RETURNS TRAIN DATALOADER AND EVALUATION DATALOADER GIVEN DATASETS\n",
        "def get_dataloaders(train_data, eval_data, batch_size=8):\n",
        "\n",
        "  train_dataset = CustomDataset(train_data[\"words\"], train_data[\"ner\"])\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  eval_dataset = CustomDataset(eval_data[\"words\"], eval_data[\"ner\"])\n",
        "  eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_dataloader, eval_dataloader"
      ],
      "metadata": {
        "id": "AVvq0hoG6mJH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO TRAIN THE MODEL\n",
        "\n",
        "# NOTE: WE ARE NOT USING DEVELOPMENT SET AS PER ASSIGNMENT REQUIREMENT\n",
        "def train(train_dataloader, model, lr=1e-5, epochs=2):\n",
        "  # use the GPU\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "  else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "  # use device for training model\n",
        "  model.to(device)\n",
        "\n",
        "  # init optimizer\n",
        "  optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "  # start training for each epoch and bring model to train mode\n",
        "  model.train()\n",
        "\n",
        "  # train for each epoch\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # train for each batch in dataloader\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      b_labels = batch[\"labels\"].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids=batch['ids'].to(device), attention_mask=batch['mask'].to(device), labels=batch['labels'].to(device))\n",
        "      logits = outputs.logits\n",
        "      loss = outputs.loss\n",
        "      train_loss += loss\n",
        "\n",
        "      predictions = torch.argmax(logits, dim=2)\n",
        "      all_predictions.append(predictions.cpu().numpy())\n",
        "      all_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # Concatenate predictions and labels from all batches\n",
        "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Calculate and print metrics for current epoch\n",
        "    train_accuracy = accuracy_score(all_predictions.flatten(), all_labels.flatten())\n",
        "    train_f1_macro = f1_score(all_predictions.flatten(), all_labels.flatten(), average='macro')\n",
        "    train_f1_micro = f1_score(all_predictions.flatten(), all_labels.flatten(), average='micro')\n",
        "\n",
        "    # print metrics for current epoch\n",
        "    print(f\"epoch: {epoch}, train_loss: {train_loss}, train_accuracy: {train_accuracy}, f1_macro: {train_f1_macro}, f1_micro: {train_f1_micro}\")\n",
        "\n",
        "  # return model to evaluate\n",
        "  return model"
      ],
      "metadata": {
        "id": "af-yxBra_iZW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO EVALUATE THE TRAINED MODEL USING EVALUATION DATASET\n",
        "def evaluate(eval_dataloader, model):\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "  else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "  # use device for training model\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "      outputs = model(input_ids=batch['ids'].to(device), attention_mask=batch['mask'].to(device), labels=batch['labels'].to(device))\n",
        "      logits = outputs.logits\n",
        "      b_labels = batch[\"labels\"].to(device)\n",
        "      predictions = torch.argmax(logits, dim=2)\n",
        "      all_predictions.append(predictions.cpu().numpy())\n",
        "      all_labels.append(b_labels.cpu().numpy())\n",
        "    # Concatenate predictions and labels from all batches\n",
        "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Calculate and print metrics for evalutation\n",
        "    eval_accuracy = accuracy_score(all_predictions.flatten(), all_labels.flatten())\n",
        "    eval_f1_macro = f1_score(all_predictions.flatten(), all_labels.flatten(), average='macro')\n",
        "    eval_f1_micro = f1_score(all_predictions.flatten(), all_labels.flatten(), average='micro')\n",
        "\n",
        "    # print metrics for evaluation\n",
        "    print(f\"eval_accuracy: {eval_accuracy}, eval_f1_macro: {eval_f1_macro}, eval_f1_micro: {eval_f1_micro}\")"
      ],
      "metadata": {
        "id": "pWRiogfM1jhF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 3 fined-tuned versions\n",
        "\n",
        "In this section, we will use all the functions above to train 3 fined-tuned versions of NER Bert Model:\n",
        "1. Fine-tuned with 1,000 sentences\n",
        "2. Fine-tuned with 3,000 sentences\n",
        "3. Fine-tuned with 3,000 sentences and frozen embeddings\n",
        "\n",
        "We will predict each fined-tuned model with evaluation set."
      ],
      "metadata": {
        "id": "TknLLbks10m0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning Bert model with 1000 sentences"
      ],
      "metadata": {
        "id": "UixDg6RnD6NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model on dataset 1 with 1000 sentences on training set\n",
        "\n",
        "# get dataloaders from train dataset 1 and evaluation dataset\n",
        "train_dataloader, eval_dataloader = get_dataloaders(train_dataset_1, eval_dataset_1)\n",
        "\n",
        "# train model\n",
        "model = train(train_dataloader, model, epochs=6)\n",
        "\n",
        "# evaluate the model\n",
        "evaluate(eval_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wlQ4cdRH3aN",
        "outputId": "a165bc03-a2b6-4046-dd5c-c0259ee0da8f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, train_loss: 76.79634857177734, train_accuracy: 0.914, f1_macro: 0.10632898678787456, f1_micro: 0.914\n",
            "epoch: 1, train_loss: 9.149255752563477, train_accuracy: 0.987796875, f1_macro: 0.24846524497127004, f1_micro: 0.987796875\n",
            "epoch: 2, train_loss: 6.913466453552246, train_accuracy: 0.987796875, f1_macro: 0.24846524497127004, f1_micro: 0.987796875\n",
            "epoch: 3, train_loss: 5.8263983726501465, train_accuracy: 0.9880703125, f1_macro: 0.28037858731238996, f1_micro: 0.9880703125\n",
            "epoch: 4, train_loss: 4.822601318359375, train_accuracy: 0.98846875, f1_macro: 0.2651312003332154, f1_micro: 0.98846875\n",
            "epoch: 5, train_loss: 3.9514920711517334, train_accuracy: 0.9891328125, f1_macro: 0.2625912239990979, f1_micro: 0.9891328125\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n",
            "eval_accuracy: 0.9891796875, eval_f1_macro: 0.1989294321592089, eval_f1_micro: 0.9891796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning Bert model with 3000 sentences"
      ],
      "metadata": {
        "id": "CXjegSQzDxkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model on dataset 2 with 3000 sentences on training set\n",
        "\n",
        "# Load pretrained Hugging Face BERT-base model for multilingual languages\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "model = BertForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "\n",
        "# get dataloaders from train dataset 2 and evaluation dataset\n",
        "train_dataloader, eval_dataloader = get_dataloaders(train_dataset_2, eval_dataset_1)\n",
        "\n",
        "# train model\n",
        "model = train(train_dataloader, model, epochs=6)\n",
        "\n",
        "# evaluate the model\n",
        "evaluate(eval_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_kFf1EiD-ck",
        "outputId": "751a6ba3-55e4-467d-f888-4559d550e534"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, train_loss: 91.99706268310547, train_accuracy: 0.9635494791666667, f1_macro: 0.1092664531438465, f1_micro: 0.9635494791666667\n",
            "epoch: 1, train_loss: 15.411498069763184, train_accuracy: 0.9885338541666666, f1_macro: 0.27934313252576626, f1_micro: 0.9885338541666666\n",
            "epoch: 2, train_loss: 9.888833999633789, train_accuracy: 0.9908802083333333, f1_macro: 0.40433674555348115, f1_micro: 0.9908802083333333\n",
            "epoch: 3, train_loss: 7.431866645812988, train_accuracy: 0.99259375, f1_macro: 0.42701374830532196, f1_micro: 0.99259375\n",
            "epoch: 4, train_loss: 5.436659812927246, train_accuracy: 0.9943515625, f1_macro: 0.7616866637799624, f1_micro: 0.9943515625\n",
            "epoch: 5, train_loss: 4.195357322692871, train_accuracy: 0.9956302083333334, f1_macro: 0.6535606874563324, f1_micro: 0.9956302083333334\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n",
            "eval_accuracy: 0.99217578125, eval_f1_macro: 0.3846825173514158, eval_f1_micro: 0.99217578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning Bert model with 3000 sentences and frozen embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "8cIn_tv2FDJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model on dataset 2 with 3000 sentences on training set and frozen embeddings\n",
        "\n",
        "# Load pretrained Hugging Face BERT-base model for multilingual languages\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "model = BertForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "\n",
        "# Freeze the embeddings\n",
        "for param in model.base_model.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Verify that embeddings are frozen\n",
        "for param in model.base_model.embeddings.parameters():\n",
        "    print(param.requires_grad)\n",
        "\n",
        "# get dataloaders from train dataset 2 and evaluation dataset\n",
        "train_dataloader, eval_dataloader = get_dataloaders(train_dataset_2, eval_dataset_1)\n",
        "\n",
        "# train model\n",
        "model = train(train_dataloader, model, epochs=6)\n",
        "\n",
        "# evaluate the model\n",
        "evaluate(eval_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t4zKrueFJ5i",
        "outputId": "d9845331-22b7-47a7-8a12-bcc2153e382f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, train_loss: 88.36167907714844, train_accuracy: 0.9634427083333333, f1_macro: 0.10930330459257781, f1_micro: 0.9634427083333333\n",
            "epoch: 1, train_loss: 15.82790470123291, train_accuracy: 0.9884322916666667, f1_macro: 0.2576432522887539, f1_micro: 0.9884322916666667\n",
            "epoch: 2, train_loss: 10.529343605041504, train_accuracy: 0.9903619791666667, f1_macro: 0.31177249653560485, f1_micro: 0.9903619791666667\n",
            "epoch: 3, train_loss: 7.634766578674316, train_accuracy: 0.9925651041666667, f1_macro: 0.5082527399088246, f1_micro: 0.9925651041666667\n",
            "epoch: 4, train_loss: 5.717084884643555, train_accuracy: 0.994078125, f1_macro: 0.4957779232048721, f1_micro: 0.994078125\n",
            "epoch: 5, train_loss: 4.6991987228393555, train_accuracy: 0.9953098958333333, f1_macro: 0.6432272289646924, f1_micro: 0.9953098958333333\n",
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n",
            "eval_accuracy: 0.99212109375, eval_f1_macro: 0.3448872447430522, eval_f1_micro: 0.99212109375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5CTSFhnFiis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}